# Discussion

## Scaled MinHash offers benefits, drawbacks vs regular MinHash

TODO: add abundance tracking in to either methods or results; gather bench?

_Scaled MinHash_ is an implementation of ModHash using concepts from
MinHashing. _Scaled MinHash_ sketches support a variety of options
that are convenient for compositional queries - most specifically,
containment, but also guarantees on hash occurrence, streaming, hash
removal, abundance tracking, and downsampling.  Importantly, _Scaled
MinHash_ sketches can be generated once for large data sets and then used
for containment searches after that, unlike CMash and mash screen.

In exchange for these features, _Scaled MinHash_ sketches have limited
sensitivity for small queries and are bounded in size by H/s, which
is usually quite large - so, practically speaking, they grow unbounded
wtih the input size.

Once a Scaled MinHash is calculated there are many operation that can
be applied without depending on the original data, saving storage
space and allowing scaling analysis to thousands of datasets.  Most of
these operations are also possible with MinHash and ModHash, with
caveats.  One example of these operations is \emph{downsampling}: the
contiguous value range for Scaled MinHash sketches allow deriving
$\mathbf{SCALED}_{s'}(W)$ sketches for any $s' \ge s$ using only
$\mathbf{SCALED}_{s}(W)$.  MinHash and ModHash can also support this
operation, as long as $n' \le n$ and $m'$ is a multiple of $m$.

Because Scaled MinHash sketches collect any value below a threshold
this also guarantees that once a value is selected it is never
discarded.  This is useful in streaming contexts: any operations that
used a previously selected value can be cached and updated with new
arriving values.  $\mathbf{MOD}_m(W)$ has similar properties, but this
is not the case for $\mathbf{MIN}_n(W)$, since after $n$ values are
selected any displacement caused by new data can invalidate previous
calculations.

Abundance tracking is another extension to MinHash sketches, keeping a
count of how many times a value appeared in the original data.  This
allows filtering for low-abundance values, as implemented in Finch
[@bovee_finch:_2018], another MinHash sketching software for genomics.
Filtering values that only appeared once was implemented before in
Mash by using a Bloom Filter and only adding values after they were
seen once, with later versions also implementing an extra counter
array to keep track of counts for each value in the MinHash.

TODO: discuss here how abundance tracking in MinHash is not "correct",
because it is not a proper weighted subsample of the data?
Note that Scaled MinHash is a proper weighted subsample.

Other operations are adding and subtracting hash values from a Scaled
MinHash sketch, allowing post-processing and filtering.  Although
possible for $\mathbf{MIN}_n(W)$, in practice this requires
oversampling (using a larger $n$) to account for possibly having less
than $n$ values after filtering (the approach taken by Finch
[@bovee_finch:_2018]).

### Scaled minhash has limitations vs regular minhash

virus, etc. (could go in first discussion section, but also deserves
to be highlighted)

## Gather works surprisingly well and matches simple data structures

We next ask, what is a minimal collection of genomes necessary to
explain the content of a metagenome? This problem can be framed
bioinformatically as asking, for a given metagenome $M$, what is the
smallest collection of genomes in the database $D$ to which all mappable
reads in $M$ will be mapped. That is, what is the smallest collection of
genomes that explain the observed reads, absent reads for which there is
no reference genome?

If we treat both $M$ and all the genomes in $D$ as collections of
k-mers, this problem can be framed as a minimal set covering problem: find the
smallest set $\{ G_n \}$ of genomes in $D$ such that
$$k(M) \cap k(D) = \bigcup_n \{ k(M) \cap k(G_n) \} $$

The gather algorithm implements the greedy min-set-cover solution to this
problem; this is a a known polynomial-time approximation algorithm.

Taxonomic results suggest that this is a pretty good approach.

Our implementation of gather does not currently select the set of
smallest genomes, but rather the smallest set of genomes. If there are
two genomes with equal containment of the k-mers, it is arbitrary as
to which one is chosen.

Note that here we are providing one approach / approximation (Scaled
MinHash containment) with one shingling approach (k-mers) to tackle
metagenome composition for mapping and taxonomy. The min-set-cover
approach could be used with exact containment, and/or with other
shingling approaches.

gather is a straightforward algorithm for "decomposing" compositional
data. It can take advantage of efficient data structures for containment
because it's "just" k-mers.

`gather` is a new method for decomposing datasets into its components
that outperforms current method when using synthetic datasets with
known composition.  By leveraging _Scaled MinHash_ sketches and
efficient indexing data structures it can scale the number of
reference datasets used by over an order of magnitude when compared to
existing methods.

The `gather` approach differs from previous methods by considering the
*co-occurrence* of $k$-mers between the query and a database sketch as
a strong signal that the k-mers originate from that database sketch.

xx can we guess at places where gather would break?

Other containment estimation methods such as
_CMash_ [@koslicki_improving_2019] and _mash screen_ [@ondov_mash_2019],
can also implement `gather`.
<!-- David comment: "CMash does kinda, but uses unique k-mers instead
of removing matches like gather does. CMash commit
https://github.com/dkoslicki/CMash/commit/de7bdd6fa --> Running a
search requires access to the original dataset (_mash screen_) for the
query, or a Bloom Filter derived from the original dataset (_CMash_),
and when the collection of reference sketches is updated the Bloom
Filter from _CMash_ can be reused, but _mash screen_ needs access to
the original dataset again.

(Maybe already covered above, or maybe should be moved to "gather can
be applied to all the data"?)
Since _Scaled MinHash_ sketches allow using the sketch directly for
`gather`, which are a fraction of the original data in size and also
allow enumerating all the elements, an operation not possible with
Bloom Filters, they can be stored and reused for large collections of
sequencing datasets, including public databases like the Sequence Read
Archive [@leinonen_sequence_2011].  A service that calculate these
_Scaled MinHash_ sketches and make them available can improve
discoverability of these large collections, as well as support future
use cases derived from other _Scaled MinHash_ features.

## Taxonomy results are excellent.

Discuss vs LCA/saturation, slash reference the LCA-has-limits/k-mers
saturate paper

Mix and match taxonomies is easy b/c we anchor to genomes.

Compared to previous taxonomic profiling methods, _Scaled MinHash_ can
also be seen as a mix of two other approaches: It uses exact $k$-mer
matching and assignment, and the $k$-mers selected by the MinHashing
process are equivalent to implicitly-defined markers.  It differs from
previous approaches because only a subset of the $k$-mer composition
is used for matching, and traditional gene markers are explicitly
chosen due to sequence conservation and low mutation rates, while
MinHashing $k$-mers generates a randomized, but consistent across
datasets, set of marker $k$-mers.

## Algorithm is simple, computational performance is great

Performant implementation in sourmasha Python API for data exploration
and methods prototyping.

### Gather can be applied to all the data.

<!-- Scaling to large collections of references --> Taxonomic
profiling is fundamentally limited by the availability of reference
datasets, even if new reference datasets can be derived from
clustering possible organisms based on sequence data in metagenomes
[@milanese_microbial_2019].  `gather` as implemented in `sourmash` is
a method that can scale to increasingly larger collections of datasets
due to multiple reasons:

  - containment and similarity estimation with _Scaled MinHash_
    sketches has lower computational requirements than alignment over
    all reads of a dataset;

  - since _Scaled MinHash_ sketches use a subset of the $k$-mer
    composition, they also scale better than full $k$-mer composition
    representations, requiring less space and reducing the number of
    elements to be computed;

  - querying multiple databases can be done independently, avoiding
    the need to merge, update or reprocess databases when new datasets
    are available.  A new database with the new datasets can be
    constructed and queried together with previous ones.

<!-- TODO to make this point I need more info about the other
databases used...  I don't think they were calculated from the refseq
snapshot https://github.com/CAMI-challenge/data/issues/2 These aspects
allowed the `sourmash` database to be include the largest number of
reference datasets of all methods compared, -->

<!-- dependency on taxonomic assignments -->

Taxonomic profiling in `sourmash` is implemented as an extra step on
top of `gather` results.  Because these steps are independent of the
dataset assignment that `gather` generates, updates to the taxonomy
don't require re-executing `gather`, since the taxonomic information
can be derived from the same dataset identifier (but potentially with
a new associated taxonomic ID).  This allows using new taxonomies
derived from the same underlying datasets [@parks_standardized_2018],
as well as updates to the original taxonomy used before.

<!-- Benchmarking --> Despite improvements to standardization and
reproducibility of previous analysis, benchmarking taxonomic profiling
tools is still challenging, since tools can generate their reference
databases from multiple sources and choosing only one source can bias
or make it impossible to evaluate them properly.  This is especially
true for real metagenomic datasets derived from samples collected from
soil and marine environments, where the number of unknown organisms is
frequently larger than those contained in reference databases.  With
the advent of metagenome-assembled genomes (MAGs) there are more
resources available for usage as reference datasets, even if they are
usually incomplete or draft quality.  `sourmash` is well positioned to
include these new references to taxonomic profiling given the minimal
requirements (a _Scaled MinHash_ sketch of the original dataset) and
support for indexing hundreds of thousands of datasets.

### Limitations of gather

`gather` as implemented in `sourmash` has the same limitations as
_Scaled MinHash_ sketches, including reduced sensitivity to small
genomes/sequences such as viruses.  _Scaled MinHash_ sketches don't
preserve information about individual sequences, and short sequences
using large scaled values have increasingly smaller chances of having
any of its $k$-mers (represented as hashes) contained in the sketch.
Because it favors the best containment, larger genomes are also more
likely to be chosen first due to their sketches have more elements,
and further improvements can take the size of the match in
consideration too.  Note that this is not necessarily the _similarity_
$J(A, B)$ (which takes the size of both $A$ and $B$), but a different
calculation that normalizes the containment considering the size of
the match.

`gather` is also a greedy algorithm, choosing the best containment
match at each step.  Situations where multiple matches are equally
well contained or many datasets are very similar to each other can
complicate this approach, and additional steps must be taken to
disambiguate matches.  The availability of abundance counts for each
element in the _Scaled MinHash_ is not well explored, since the
process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the
count in the match).
<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

### Future directions for gather

In this chapter `gather` is described in terms of taxonomic profiling
of metagenomes.  That is one application of the algorithm, but it can
applied to other biological problems too.  If the query is a genome
instead of a metagenome, `gather` can be used to detect possible
contamination in the assembled genome by using a collection of genomes
and removing the query genome from it (if it is present).  This allows
finding matches that contain the query genome and evaluating if they
agree at specific taxonomic rank, and in case of large divergence (two
different phyla are found, for example) it is likely to indicative
that the query genome contains sequences from different organisms.
This is especially useful for quality control and validation of
metagenome-assembled genomes (MAGs), genomes assembled from reads
binned and clustered from metagenomes, as well as a verification
during submission of new assembled genomes to public genomic databases
like GenBank.

Improvements to the calculation of _Scaled MinHash_ sketches can also
improve the taxonomic profiling use case.  Exact $k$-mer matching is
limited in phylogenetically distant organisms, since small nucleotide
differences lead to distinct $k$-mers, breaking homology
assumptions. <!-- TODO verify/cite? --> Different approaches for
converting the datasets into a set to be hashed (_shingling_) than
computing the nucleotide $k$-mer composition, such as spaced $k$-mers
[@leimeister_fast_2014] and minimizers [@roberts_reducing_2004] and
alternative encodings for the nucleotides using 6-frame translation to
amino acid [@gish_identification_1993] or other reduced alphabets
[@peterson_reduced_2009], can allow comparisons on longer evolutionary
distances and so improve taxonomic profiling by increasing the
sensitivity of the containment estimation.  These improvements don't
fundamentally change the `gather` method, since it would still be
based on the same *containment* and *remove element* operations, but
show how `gather` works as a more general method that can leverage
characteristics from different building blocks and explore new or
improved use cases.

`gather` is a new method for decomposing datasets into its components
with application in biological sequencing data analysis (taxonomic
profiling) that can scale to hundreds of thousands of reference
datasets with computational resources requirements that are accessible
to a large number of users when used in conjunction with _Scaled
MinHash_ sketches and efficient indices such as _LCA_ and _MHBT_.  It
outperforms current methods in community-develop benchmarks, and opens
the way for new methods that explore a top-down approach for profiling
microbial communities, including further refinements that can resolve
larger evolutionary distances and also speed up the method
computationally.

### XXX SBT and LCA indices

_Scaled MinHash_ sketches are fundamentally a subset of the $k$-mer
composition of a dataset, and so any of the techniques described in
[@marchet_data_2019] are potential candidates for improving current
indices or implementing new ones.  The MHBT index can be improved by
using more efficient representations for the internal nodes
[@solomon_improved_2017] and constructing the MHBT by clustering
[@harris_improved_2018], and the LCA index can use more efficient
storage of the list of signatures IDs by representing the list as
colors [@pandey_mantis:_2018].  The memory consumption of the LCA
index can also be tackled by implementing it in external memory using
memory-mapped files, letting the operating system cache and unload
pages as needed.

Current indices are also single-threaded, and don't benefit from
multicore systems.  Both indices can be used in parallel by loading as
read-only and sharing for multiple searches, but is is also possible
to explore parallelization for single queries by partitioning the LCA
and assigning each partition to a thread, as well as using a
work-stealing thread pool for expanding the search frontier in the
MHBT in parallel.  In any case, the current implementations serve as a
baseline for future scalability and can be used to guide optimization
and avoid extraneous overhead and common failings of such projects
[@mcsherry_scalability_2015].

### Database types work well

"online" approaches

`sourmash gather`, the command-line interface that adds further user
experience improvements to the API level, also allows passing multiple
indices to be searched, providing incremental support for rerunning
with additional data without having to recompute, merge or update the
original databases.

Some limitations of gather and database types (equal results can be
hard to detect efficiently with current SBT implementation)

The Linear index is appropriate for operations that must check every
signature, since it doesn't have any indexing overhead.  An example is
building a distance matrix for comparing signatures all-against-all.
Search operations greatly benefit from extra indexing structure.  The
MHBT index and $k$-mer aggregative methods in general are appropriate
for searches with query thresholds, like searching for similarity or
containment of a query in a collection of datasets.  The LCA index and
color aggregative methods are appropriate for querying which datasets
contain a specific query $k$-mer.

As implemented in sourmash, the MHBT index is more memory efficient
because the data can stay in external memory and only the tree
structure for the index need to be loaded in main memory, and data for
the datasets and internal nodes can be loaded and unloaded on demand.
The LCA index must be loaded in main memory before it can be used, but
once it is loaded it is faster, especially for operations that need to
summarize $k$-mer assignments or require repeated searches.

Due to these characteristics, and if memory usage is not a concern,
then the LCA index is the most appropriate choice since it is faster.
The MHBT index is currently recommended for situations where memory is
limited, such as with smaller scaled values ($s\le2000$) that increase
the size of signatures, or when there are a large number (hundreds of
thousands or more) of datasets to index.

### Converting between indices

Both MHBT and LCA index can recover the original sketch collection.
In the MHBT case, it outputs all the leaf nodes.  In the LCA index, it
reconstruct each sketch from the hash-to-dataset-ID mapping.  This
allows trade-offs between storage efficiency, distribution, updating
and query performance.

Because both are able to return the original sketch collection, it is
also possible to convert one index into the other.

### gather Conclusion

_Scaled MinHash_ sketches allow scaling analysis to thousands of
datasets, but efficiently searching and sharing them can benefit from
data structures that index and optimize these use cases.  This chapter
introduces an index abstraction that can be trivially implementing
using a list of sketches (_Linear index_) and more advanced
implementations based on inverted indices (_LCA index_) and
hierarchical indices (_MHBT_) providing options for fast and
memory-efficient operations, as well as making it easier to share and
analyze collections of sketches.  All these functionalities are
implemented in sourmash.

## Limitations and future directions

(For _Scaled MinHash_, `gather`, and taxonomy. Move where? Conclusions?)

(From David Koslicki)
Gotchas:

* Lack of sensitivity for small queries
* Potentially large sketch sizes

And a couple other that I’ve tentatively/mathematically observed:

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)

_Scaled MinHash_ sketches are fundamentally a subset of the $k$-mer
composition of a dataset, and so any of the techniques described in
[@marchet_data_2019] are potential candidates for improving current
indices or implementing new ones.  The MHBT index can be improved by
using more efficient representations for the internal nodes
[@solomon_improved_2017] and constructing the MHBT by clustering
[@harris_improved_2018], and the LCA index can use more efficient
storage of the list of signatures IDs by representing the list as
colors [@pandey_mantis:_2018].  The memory consumption of the LCA
index can also be tackled by implementing it in external memory using
memory-mapped files, letting the operating system cache and unload
pages as needed.

Current indices are also single-threaded, and don't benefit from
multicore systems.  Both indices can be used in parallel by loading as
read-only and sharing for multiple searches, but is is also possible
to explore parallelization for single queries by partitioning the LCA
and assigning each partition to a thread, as well as using a
work-stealing thread pool for expanding the search frontier in the
MHBT in parallel.  In any case, the current implementations serve as a
baseline for future scalability and can be used to guide optimization
and avoid extraneous overhead and common failings of such projects
[@mcsherry_scalability_2015].

_Scaled MinHash_ sketches allow scaling analysis to thousands of
datasets, but efficiently searching and sharing them can benefit from
data structures that index and optimize these use cases.  This chapter
introduces an index abstraction that can be trivially implementing
using a list of sketches (_Linear index_) and more advanced
implementations based on inverted indices (_LCA index_) and
hierarchical indices (_MHBT_) providing options for fast and
memory-efficient operations, as well as making it easier to share and
analyze collections of sketches.  All these functionalities are
implemented in `sourmash`, a software package exposing these features
as a command-line program as well as a Python API for data exploration
and methods prototyping.

These indices also serve as another set of building blocks for
constructing more advanced methods for solving other relevant
biological problems like taxonomic profiling, described in Chapter
[3](#chp-gather), and approaches for increasing the resilience and
shareability of biological sequencing data, described in Chapter
[5](#chp-decentralizing).

