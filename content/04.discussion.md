# Discussion

## Scaled MinHash provides efficient containment queries for large data sets.

_Scaled MinHash_ is an implementation of ModHash that uses the bottom
hashing concept from MinHash to support containment operations. In
brief, all elements in the set to be sketched are hashed, and any hash
below a certain fixed boundary value are kept. This fixed boundary
value is determined by the desired accuracy for the sketch
representation.

Intuitively, _Scaled MinHash_ performs a density sampling at a rate of 1
$k$-mer per $s$ distinct k-mers seen, where $s$ is the size of the
hash space divided by the boundary value used in creating the
sketch. This is a type of lossy compression, with a fixed compression
ratio of $s$: for values of $s$ used here ($s \approx
1000$), data sets are reduced in size 1000-fold.

Unlike MinHash, _Scaled MinHash_ supports containment analysis between
sets of very different sizes, and here we demonstrate that it can be
used efficiently and effectively for compositional analysis of shotgun
metagenome data sets with k-mers. In particular, _Scaled MinHash_ is
competitive in accuracy with extant MinHash-based techniques for
containment analysis, while also supporting Jaccard similarity.
Footnote: We note that others have also applied the ModHash concept to
genomic data; see, for example, Durbin's "modimizer"
[@https://github.com/richarddurbin/modimizer].

_Scaled MinHash_ offers several conveniences over _MinHash_.
No hash is ever removed from a Scaled MinHash during
construction; while this means that sketches grow proportionally to
the number of distinct k-mers in the sampled data set, sketches _also_
support many operations without needing to revisit the original data
set. This is in contrast to MinHash, which requires auxiliary data
structures for many operations - most especially, containment
operations (cite CMash and mash screen).  Thus Scaled MinHash
sketches serve as distributed compressed indices for the original
content for a much broader range of operations than MinHash.

Because _Scaled MinHash_ sketches collect all hash values below a
fixed threshold, they support streaming analysis of sketches: any
operations that used a previously selected value can be cached and
updated with newly arriving values.  ModHash has similar
properties, but this is not the case for MinHash, since
after $n$ values are selected any displacement caused by new data can
invalidate previous calculations.

Scaled MinHash also directly supports the addition and subtraction of
hash values from a sketch, allowing post-processing and filtering without
revisiting the original data set. This includes unions and intersections.
Although possible for MinHash, in practice this requires
oversampling (using a larger $n$) to account for possibly having less
than $n$ values after filtering (the approach taken by Finch
[@bovee_finch:_2018]).

Another useful operation available on _Scaled MinHash_ sketches is
*downsampling*: the contiguous value range for Scaled MinHash sketches
allow deriving $\mathbf{SCALED}_{s'}(W)$ sketches for any $s' \ge s$
using only $\mathbf{SCALED}_{s}(W)$.  MinHash and ModHash can also
support this operation in limited circumstances, when $n' \le n$
and $m'$ is a multiple of $m$.  Note also that _Scaled MinHash_ and
regular MinHash sketches can be converted between each other when compatible
hashing schemes are used, and when (insert math here about boundary
values etc.)

When the multiplicity of hashes in the original data is retained,
_Scaled MinHash_ sketches can be filtered on abundance.  This allows
removing low-abundance values, as implemented in Finch
[@bovee_finch:_2018], another MinHash sketching software for genomics.
Filtering values that only appear once was implemented in Mash by
using a Bloom Filter and only adding values after they were seen once,
with later versions also implementing an extra counter array to keep
track of counts for each value in the MinHash.  These operations can
be done in _Scaled MinHash_ without auxiliary data structures.

Finally, because Scaled MinHash are collections of
hashes, existing k-mer indexing approaches can be applied to the
sketches to provide fast database search of these indices; a number of
indexing operations, including Sequence Bloom Trees and reverse indices,
are provided in the sourmash software.

In exchange for these many conveniences, _Scaled MinHash_ sketches
have limited sensitivity for small data sets (data set size
approx. $s$) and are only bounded in size by $H/s$, which is typically
quite large.  The limited sensitivity of sketches may affect the
sensitivity of gene- and viral genome-sized queries, but at $s=1000$
we see comparable accuracy and sketch size to MinHash for bacterial
genome comparisons (Figure XXX, currently 1).

## Minimum set covers can be used for accurate compositional analysis of metagenomes.

Many metagenome content analysis approaches use reference genomes to
interpret metagenome content, but most such approaces rely on a
curated list of non-redunant genomes from a much larger database
(e.g. Humann3/biobakery, cite).
Here, we frame the computational
challenge of discovering the appropriate reference genomes for a set
of metagenome reads as a min-set-cov problem, in which we seek a
*minimum* set of reference genomes necessary to account for all k-mers
shared between the metagenome and the reference database. We show that
this can be resolved efficiently for real-world data sets using a
greedy algorithm; using _Scaled MinHash_, we provide an approach
that readily scales to 700,000 genomes on current hardware (performance
in appendix).

The development of a small list of relevant genomes is particularly
useful for large reference databases containing many redundant
genomes; for example, in Table XXX, we show that for one particular
mock community, we can select a minimum metagenome cover of 19 genomes
for a metagenome that contains matches to over 400,000 genomes total.
This minimum metagenome cover can then be used as inputs for taxonomic
analysis and/or read mapping.

Our comparison of hash-based estimation of containment to mapping
results in Figure @fig:gather shows that this approach is an accurate
proxy for systematic mapping.  In particular, hash-based estimation of
containment closely matches actual read mapping performance.

This min-set-cov approach for assigning genomes to metagenomes using
k-mers differs substantially from extant k-mer and mapping-based
approaches.  LCA-based approaches such as Kraken label individual
k-mers based on taxonomic lineages in a database, and then use the
resulting database of annotated k-mers to assign taxonomy to
reads. Mapping- and homology-based approaches such as Diamond or @@@
use read mapping to genomes or read alignment to gene sequences in
order to assign taxonomy and function. These approaches typically
focus on individual k-mers or reads. This is in contrast to the greedy
min-set-cov approach described here, which looks at the entire
collection of reads/k-mers and assigns them _in aggregate_ to the
_best_ genome match. Based on the CAMI community benchmarks, this
greedy approach to content analysis is competitive in accuracy to
other techniques.

Our implementation of the min-set-cov algorithm in sourmash also
readily supports custom reference databases as well as updating
minimum set covers with the addition of new reference genomes. When
updating set covers, the first stage of calculating overlaps can be
done on only the new genomes (Column XYZ of Table ZZZ), while the
actual calculation of the minimum set cover must be redone.

With our implementation of min-set-cov on top of _Scaled MinHash_,
there is a loss of resolution for very closely related genomes,
because distinct hashes will not be chosen for them.
However, other data structures can be easily used for min-set-cov:
any data structure supporting both the _containment_ $C(A, B) =
\frac{\vert A \cap B \vert }{\vert A \vert}$ and _remove elements_
operations can be used to implement the greedy approximation algorithm
(ref algorithm in results section 1).
For example, a simple _set_
of the $k$-mer composition of the query supports element removal, and
calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the _Counting
Quotient Filter_ [@pandey_general-purpose_2017] can also be used, with
the benefit of reduced storage and memory usage.  Moreover, the
collection of datasets can be implemented with any data structure that
can do containment comparisons with the query data structure. It
is important to have performant containment searches, since
`gather` may run `FindBestContainment` many times.

This approach is reference-based, and hence is very dependent on the
reference database. In particular, in many cases the exact reference
strains present in the metagenome will not be present in the
database. This manifests in two ways in Figure @fig:minhash. First,
there is a systematic mismatch between the hash content and the
mapping content (green line), because mapping software is more
permissive in the face of small variants than k-mer-based exact
matching. Moreover, many of the lower rank genomes in the plot are
from the same species but different *strains* as the higher ranked
genomes, suggesting that strain-specific portions of the reference are
being utilized for matching at lower ranks. In reality, there will
usually be a different mixture of strains in the metagenome than in
the reference database. Approaches such as spacegraphcats may help
resolve this by adapting old references. @cite.

xx can we guess at places where gather would break? One is equivalent
containment/different genome sizes.

## Minimum metagenome covers support accurate and flexible taxonomic conversation

Once the min-set-cov approach has identified reference genomes, we can
build a taxonomic classifier for metagenome content by simply
reporting the taxonomies of the constitutent genomes, aggregated at
the relevant taxonomic level using an LCA approach.  Our initial
taxonomic benchmarking shows that this approach is competitive for all
metrics across all taxonomic levels (Figure XYZ).

One convenient feature of this approach to taxonomic analysis is that
new or changed taxonomies can be readily incorporated by assigning
them directly to genome identifiers; the majority of the computational
work is involved in finding the reference genomes, which can have
assignments in different taxonomic frameworks. For example, sourmash
already supports GTDB natively, and will also support the emerging
LINS framework (cite, cite).  sourmash can also readily incorporate
updates to taxonomies, e.g. the frequent updates to the NCBI taxonomy,
without requiring expensive reanalysis of the primary metagenome data
or even redoing the min-set-cov computation.  Interestingly, the
framing of taxonomic classification as a minimum set cover problem
may also avoid the loss of taxonomic resolution that affects
k-mer- and read-based approaches (cite cite); this is because we apply
LCA _after_ reads and k-mers have been assigned to individual genomes.

Finally, as the underlying min-set-cov implementation supports custom
databases, it is straightforward to support taxonomic analysis using
custom databases and/or custom taxonomic assignments. Indeed, sourmash
already supports this natively.

## Algorithm is simple, computational performance is great

The algorithms underlying both _Scaled MinHash_ and the greedy
min-set-cov solution are simple to describe and straightforward to
implement.  This increases the likelihood of correct implementation,
provides opportunities for independent optimization of data structures,
and simplifies interoperability between different implementations.

We provide two implementations with this paper: sourmash, a fully
supported open source implementation with command-line, Python and
Rust APIs; and smol, a much shorter Rust implementation for
demonstration purposes.

## sourmash supports large scale data analysis

We provide a robust implementation in sourmash that performs well in
practice and supports a wide variety of use cases (see performacne in
appendix, docs and tutorials at sourmash.rtfd.io, installation via pip
and conda).

Taxonomic profiling is fundamentally limited by the availability of
reference datasets, even if new reference datasets can be derived from
clustering possible organisms based on sequence data in metagenomes
[@milanese_microbial_2019].  The sourmash project provides large scale
databases for NCBI and GTDB taxonomies, and supports search of all
available genomes.

## Limitations of our approach

(For _Scaled MinHash_, `gather`, and taxonomy. Move where? Conclusions?)

`gather` as implemented in `sourmash` has the same limitations as
_Scaled MinHash_ sketches, including reduced sensitivity to small
genomes/sequences such as viruses.  _Scaled MinHash_ sketches don't
preserve information about individual sequences, and short sequences
using large scaled values have increasingly smaller chances of having
any of its $k$-mers (represented as hashes) contained in the sketch.
Because it favors the best containment, larger genomes are also more
likely to be chosen first due to their sketches have more elements,
and further improvements can take the size of the match in
consideration too.  Note that this is not necessarily the _similarity_
$J(A, B)$ (which takes the size of both $A$ and $B$), but a different
calculation that normalizes the containment considering the size of
the match.

`gather` is also a greedy algorithm, choosing the best containment
match at each step.  Situations where multiple matches are equally
well contained or many datasets are very similar to each other can
complicate this approach, and additional steps must be taken to
disambiguate matches.  The availability of abundance counts for each
element in the _Scaled MinHash_ is not well explored, since the
process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the
count in the match).
<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

(From David Koslicki)
Gotchas:

* Lack of sensitivity for small queries
* Potentially large sketch sizes

And a couple other that I’ve tentatively/mathematically observed:

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)

## We belieeeeeeeve

min set cov could be applied in many more circumstances - read based analysis,
contig based analysis, maybe variant calling, etc.

Mention weighted cover cc David?


Note that here we are providing one approach / approximation (Scaled
MinHash containment) with one shingling approach (k-mers) to tackle
metagenome composition for mapping and taxonomy. The min-set-cover
approach could be used with exact containment, and/or with other
shingling approaches.

CTB: discuss centrifuge, etc. Could this be impemented on top of that?
