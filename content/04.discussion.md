# Discussion

## Scaled MinHash provides efficient containment queries for large data sets.

_Scaled MinHash_ is an implementation of ModHash that uses the bottom
hashing concept from MinHash to support containment operations. In
brief, all elements in the set to be sketched are hashed, and any hash values
below a certain fixed boundary value are kept for the sketch. This fixed boundary
value is determined by the desired accuracy for the sketch
representation.

Intuitively, _Scaled MinHash_ performs a density sampling at a rate of 1
$k$-mer per $s$ distinct k-mers seen, where $s$ is the size of the
hash space divided by the boundary value used in creating the
sketch. This is a type of lossy compression, with a fixed compression
ratio of $s$: for values of $s$ used here ($s \approx
1000$), data sets are reduced in size 1000-fold.

Unlike MinHash, _Scaled MinHash_ supports containment estimation between
sets of very different sizes, and here we demonstrate that it can be
used efficiently and effectively for compositional analysis of shotgun
metagenome data sets with k-mers. In particular, _Scaled MinHash_ is
competitive in accuracy with extant MinHash-based techniques for
containment analysis, while also supporting Jaccard similarity.[^durbin]

[^durbin]: We note that others have also applied the ModHash concept to
genomic data; see, for example, Durbin's "modimizer"
[@https://github.com/richarddurbin/modimizer].

_Scaled MinHash_ offers several conveniences over _MinHash_.  No hash
is ever removed from a _Scaled MinHash_ sketch during construction;
while this means that sketches grow proportionally to the number of
distinct k-mers in the sampled data set, sketches _also_ support many
operations - including all of the operations used in this paper -
without needing to revisit the original data set. This is in contrast
to MinHash, which requires auxiliary data structures for many
operations - most especially, containment operations (cite CMash and
mash screen).  Thus Scaled MinHash sketches serve as distributed
compressed indices for the original content for a much broader range
of operations than MinHash.

Because _Scaled MinHash_ sketches collect all hash values below a
fixed threshold, they also support streaming analysis of sketches: any
operations that used a previously selected value can be cached and
updated with newly arriving values.  ModHash has similar
properties, but this is not the case for MinHash, since
after $n$ values are selected any displacement caused by new data can
invalidate previous calculations.

Scaled MinHash also directly supports the addition and subtraction of
hash values from a sketch, allowing post-processing and filtering without
revisiting the original data set. This includes unions and intersections.
Although possible for MinHash, in practice this requires
oversampling (using a larger $n$) to account for possibly having less
than $n$ values after filtering; this approach is taken by Finch, another
MinHash sketching software for genomics [@doi:10.21105/joss.00505].

When the multiplicity of hashes in the original data is retained,
_Scaled MinHash_ sketches can be filtered on abundance.  This allows
removing low-abundance values, as implemented in Finch
[doi:10.21105/joss.00505].  Filtering values that only appear once
was implemented in Mash by using a Bloom Filter and only adding values
after they were seen once, with later versions also implementing an
extra counter array to keep track of counts for each value in the
MinHash.  These operations can be done in _Scaled MinHash_ without
auxiliary data structures.

Another useful operation available on _Scaled MinHash_ sketches is
*downsampling*: the contiguous value range for Scaled MinHash sketches
allows deriving MinHash sketches from _Scaled MinHash_ sketches
whenever the number of hashes in the _Scaled MinHash_ sketch is equal
to or greater than $n$, as long as the same hashing scheme is used.
Likewise, MinHash sketches can be converted to _Scaled MinHash_
sketches when the maximum hash value in the MinHash sketch is larger
than $s$.

Finally, because _Scaled MinHash_ sketches are simply collections of
hashes, existing k-mer indexing approaches can be applied to the
sketches to support fast search with both similarity and containment estimators; several index types,
including Sequence Bloom Trees and reverse indices, are provided in
the sourmash software.

In exchange for these many conveniences, _Scaled MinHash_ sketches
have limited sensitivity for small data sets where the k-mer
cardinality of the data set $\approx s$, and are only bounded in size
by $H/s$ (typically quite large, $\approx 2e16$).  The limited
sensitivity of sketches may affect the sensitivity of gene- and viral
genome-sized queries, but at $s=1000$ we see comparable accuracy and
sketch size to MinHash for bacterial genome comparisons (Figure
@fig:containment).

## Minimum set covers can be used for accurate compositional analysis of metagenomes.

Many metagenome content analysis approaches use reference genomes to
interpret the metagenome content, but most such approaches rely on a
curated list of non-redundant genomes from a much larger database
(e.g. bioBakery 3 selects approximately 100,000 genomes [@doi:10.7554/eLife.65088]).  Here, we search an arbitrarily large database to retrieve
a *minimum* set of reference genomes necessary to account for all k-mers
shared between the metagenome and the database. We show that
this can be resolved efficiently for real-world data sets; using _Scaled MinHash_ with a greedy min-set-cov algorithm, we provide an approach that
readily scales to 700,000 genomes on current hardware (performance in
appendix). Moreover, this procedure reduces the number of genomes
under consideration to $\approx 100$ for several mock and real
metagenomes.

The development of a small list of relevant genomes is particularly
useful for large reference databases containing many redundant
genomes; for example, in Table @tbl:genbank-cover, we show that for one particular
mock community, we can select a minimum metagenome cover of 19 genomes
for a metagenome that contains matches to over 400,000 GenBank genomes total.

This minimum metagenome cover can then be used as inputs for further
analysis, including both taxonomic content analysis and mapping
approaches.  For taxonomic analyses, we find that this approach is
competitive with other current approaches and has several additional
conveniences (discussed in detail below).  The comparison of
hash-based estimation of containment to mapping results in Figure
@fig:mapping suggests that this approach is an accurate proxy for
systematic mapping.

Our implementation of the min-set-cov algorithm in sourmash also
readily supports custom reference databases as well as updating
minimum set covers with the addition of new reference genomes. When
updating set covers with new reference genomes, the first stage of
calculating overlaps can be updated with the new genomes (Column 2 of
Table @tbl:genbank-cover), while the actual calculation of the minimum
set cover must be redone each time.

CTB TODO:

* can we guess at places where gather would break? One is equivalent
containment/different genome sizes, e.g. virus/phage contained within
other genomes.

## Minimum metagenome covers support accurate and flexible taxonomic assignment

We can build a taxonomic classifier on top of minimum set covers for metagenomes
by reporting the taxonomies of the constituent genomes, aggregated at
the relevant taxonomic level using an LCA approach.  Our initial
taxonomic benchmarking shows that this approach is competitive for all
metrics across all taxonomic levels (Figures @fig:spider and @fig:scores).

One convenient feature of this approach to taxonomic analysis is that
new or changed taxonomies can be readily incorporated by assigning
them directly to genome identifiers; the majority of the computational
work is involved in finding the reference genomes, which can have
assignments in different taxonomic frameworks. For example, sourmash
already supports GTDB [@doi:10.1093/nar/gkab776] natively, and will
also support the emerging LINS framework
[@doi:10.1094/PHYTO-07-16-0252-R].  sourmash can also readily
incorporate updates to taxonomies, e.g. the frequent updates to the
NCBI taxonomy, without requiring expensive reanalysis of the primary
metagenome data or the min-set-cov computation.

Interestingly, the framing of taxonomic classification as a minimum
set cover problem may also avoid the loss of taxonomic resolution that
affects k-mer- and read-based approaches on large databases
[@doi:10.1186/s13059-018-1554-6]; this is because we apply LCA
_after_ reads and k-mers have been assigned to individual genomes, and
choose entire *genomes* based on a greedy best-match-first approach.
This minimizes the impact of individual k-mers that may be common to
a genus or family, or were mis-assigned as a result of contamination.

Finally, as the underlying min-set-cov implementation supports custom
databases, it is straightforward to support *txonomic* analysis using
custom databases and/or custom taxonomic assignments. sourmash
already supports this functionality natively.

## Simple algorithms support performant implementations

The algorithms underlying both _Scaled MinHash_ and the greedy
min-set-cov approximation are simple to describe and straightforward
to implement.  This increases the likelihood of correct
implementation, provides opportunities for independent optimization of
data structures, and simplifies interoperability between different
implementations.

In the sourmash software package, we provide a mature and optimized
implementation that implements all of the operations above.  sourmash
performs well in practice and supports a wide variety of use cases
(CTB: see performance in appendix, docs and tutorials at
sourmash.rtfd.io, and installation instructions for pip and conda).
The sourmash project also provides large scale databases for NCBI and
GTDB taxonomies.

The approach presented here chooses arbitrarily between
matches with equivalent numbers of contained k-mers. There are specific
genomic circumstances where this approach could usefully be refined with
additional criteria. For example, if a phage genome is present in the
reference database, and is also present within one or more genomes in the
database, it may desirable to select the match with the highest
Jaccard *similarity* in order to select the phage genome directly.

## Reference dependence

The min-set-cov approach is reference-based, and hence is entirely
dependent on the reference database. In particular, in many cases the
exact reference strains present in the metagenome will not be present
in the database. This manifests in two ways in Figure
@fig:mapping. First, there is a systematic mismatch between the hash
content and the mapping content (green line), because mapping software
is more permissive in the face of small variants than k-mer-based
exact matching. Moreover, many of the lower rank genomes in the plot
are from the same species but different *strains* as the higher ranked
genomes, suggesting that strain-specific portions of the reference are
being utilized for matching at lower ranks. In reality, there will
usually be a different mixture of strains in the metagenome than in
the reference database. Methods for updating references from
metagenome data sets may provide an approach for generating
metagenome-specific references [@doi:10.1186/s13059-020-02066-4].

## Opportunities for future improvement

Implementing min-set-cov on top of _Scaled MinHash_ means
that there is a loss of resolution when choosing between very closely
related genomes, because the set of hashes chosen may not discriminate
between them.  Likewise, the potentially very large size of the sketches
may inhibit the application of this approach to metagenomes.

These limitations are not intrinsic to min-set-cov, however; 
any data structure supporting both the _containment_ $C(A, B) =
\frac{\vert A \cap B \vert }{\vert A \vert}$ and _remove elements_
operations can be used to implement the greedy approximation algorithm
(ref algorithm in results section 1).
For example, a simple _set_
of the $k$-mer composition of the query supports element removal, and
calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the _Counting
Quotient Filter_ [@doi:10.1145/3035918.3035963] can also be used, with
the benefit of reduced storage and memory usage.  Moreover, the
collection of datasets can be implemented with any data structure that
can do containment comparisons with the query data structure.

In turn, this means that limitations of our current implementation,
such as insensitivity to small genomes when $s$ is approximately the
same as the genome size, are not intrinsic to the minimum set cover
approach.

There are many other opportunities for improving on the initial explorations
above.
The availability of abundance counts for each
element in the _Scaled MinHash_ is not well explored, since the
process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the
count in the match).
<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

From DK: a couple other that I’ve tentatively/mathematically observed:

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)
  
Mention weighted set covers.

## Additional opportunities

The min-set-cov approach for assigning genomes to metagenomes using
k-mers differs substantially from extant k-mer and mapping-based
approaches for identifying relevant genomes.  LCA-based approaches
such as Kraken label individual k-mers based on taxonomic lineages in
a database, and then use the resulting database of annotated k-mers to
assign taxonomy to reads. Mapping- and homology-based approaches such
as Diamond use read mapping to genomes or read alignment to
gene sequences in order to assign taxonomy and function (cite). These
approaches typically focus on assigning *individual* k-mers or reads.
In contrast, here we analyze the entire collection of k-mers and
assigns them _in aggregate_ to the _best_ genome match.

Minimum set cover approaches may provide opportunities beyond those
discussed here. For example, read- and contig-based analysis, and analysis
and presentation of alignments, can be potentially simplified with this
approach.
