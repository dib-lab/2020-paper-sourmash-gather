# Discussion

Below, we discuss the use of FracMinHash and minimum metagenome covers
to analyze metagenome datasets.

<!-- (CTB: probably want to talk a bit about long reads below, too.) -->

## FracMinHash provides efficient containment queries for large data sets.

FracMinHash is a derivative of ModHash that uses the bottom hashing
concept from MinHash to support containment operations: all
elements in the set to be sketched are hashed, and any hash values
below a certain fixed boundary value are kept for the sketch. This
fixed boundary value is determined by the desired accuracy for the
sketch operations, with clear space/time constraint tradeoffs.

Intuitively, FracMinHash can be viewed as performing density sampling
at a rate of 1 $k$-mer per $s$ distinct k-mers seen, where $s$ is used
to define a boundary value $\frac{H}{s}$ for the bottom sketch.
FracMinHash can also be viewed as a type of lossy compression,
with a fixed compression ratio of $s$: for values of $s$ used here ($s
\approx 1000$), k-mer sets are reduced in cardinality by 1000-fold.

Unlike MinHash, FracMinHash supports containment estimation between
sets of very different sizes, and here we demonstrate that it can be
used efficiently and effectively for compositional analysis of shotgun
metagenome data sets with k-mers. In particular, FracMinHash is
competitive in accuracy with extant MinHash-based techniques for
containment analysis, while also supporting Jaccard similarity.

We note that the FracMinHash technique has been used under a number of
different names, including Scaled MinHash
[@sourmash_f1000;@luiz_thesis], universe minimizers
[@doi:10.1016/j.cels.2021.08.009], Shasta markers
[@doi:10.1038/s41587-020-0503-6], and mincode syncmers
[@doi:10.7717/peerj.10805].  The name FracMinHash was coined by
Kristoffer Sahlin in an online discussion on Twitter
[@url:https://twitter.com/krsahlin/status/1463169988689285125] and
chosen by discussants as the least ambiguous option. We use it here
accordingly.

FracMinHash offers several conveniences over MinHash.  No hash is ever
removed from a FracMinHash sketch during construction; thus sketches
grow proportionally to the number of distinct k-mers in the sampled
data set, but _also_ support many operations - including all of the
operations used here - without needing to revisit the original data
set. This is in contrast to MinHash, which requires auxiliary data
structures for many operations - most especially, containment
operations [@doi:10.1101/184150;@doi:10.1186/s13059-019-1841-x].  Thus
FracMinHash sketches serve as compressed indices for the original
content for a much broader range of operations than MinHash.

Because FracMinHash sketches collect all hash values below a fixed
threshold, they also support streaming analysis of sketches: any
operations that used a previously selected value can be cached and
updated with newly arriving values.  ModHash has similar properties,
but this is not the case for MinHash: after $n$ values are selected
any displacement caused by new data can invalidate previous
calculations.

FracMinHash also directly supports the addition and subtraction of
hash values from a sketch, allowing for limited types of
post-processing and filtering without revisiting the original data
set. This includes unions and intersections.  Although possible for
MinHash, in practice this requires oversampling (using a larger $n$)
to account for possibly having fewer than $n$ values after filtering,
e.g. see the approach taken in Finch [@finch].

When the multiplicity of hashes in the original data is retained,
FracMinHash sketches can be filtered on abundance.  This allows
removing low-abundance values, as implemented in Finch
[@finch].  Filtering values that only appear once
was implemented in Mash by using a Bloom filter and only adding values
after they were seen once; later versions also implemented an
extra counter array to keep track of counts for each value in the
MinHash.  These operations can be done in FracMinHash without
auxiliary data structures.

Another useful operation available on FracMinHash sketches is
*downsampling*: the contiguous value range for FracMinHash sketches
means that MinHash sketches can be extracted from FracMinHash sketches
whenever the size of the requested MinHash is less than the size of
the FracMinHash sketch.  Likewise, MinHash sketches can be losslessly converted
to FracMinHash sketches when the maximum hash value in the MinHash
sketch is larger than $H / s$.

Finally, because FracMinHash sketches are simply collections of
hashes, existing hash-based k-mer indexing approaches can be applied
to sketches to support fast search with both similarity and
containment estimators; several index types, including Sequence Bloom
Trees [@doi:10.1038/nbt.3442] and reverse indices, are provided in the
sourmash software.

In exchange for these many conveniences, FracMinHash sketches
have limited sensitivity for small data sets where the k-mer
cardinality of the data set $\approx s$, and are only bounded in size
by $H/s$, which is typically quite large $\approx 2e16$.  The limited
sensitivity of sketches may affect the sensitivity of gene- and viral
genome-sized queries, but at $s=1000$ we see comparable accuracy and
sketch size to MinHash for bacterial genome comparisons (Figure
@fig:containment).

<!--
Notes from DK: do these belong in this section?

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)
-->

## Minimum set covers can be used for accurate compositional analysis of metagenomes.

Many metagenome content analysis approaches use reference genomes to
interpret the metagenome content, but most such approaches rely on
starting with a list of reduced-redundancy genomes from a much larger
database (e.g. bioBakery 3 selects approximately 100,000 genomes
[@biobakery3]), which can reduce sensitivity and precision
[@metalign].  Here, we incorporate this reduction into the overall
workflow by searching the complete database for a *minimum* set of
reference genomes necessary to account for all k-mers shared between
the metagenome and the database. We show that this can be resolved
efficiently for real-world data sets; implementing a greedy
min-set-cov approximation algorithm on top of FracMinHash, we provide
an approach that readily scales to 700,000 genomes on current
hardware. We show that in practice this procedure reduces the number
of genomes under consideration to $\approx 100$ for several mock and
real metagenomes.
<!-- CTB: performance should be added to appendix -->

The development of a small list of relevant genomes is particularly
useful for large reference databases containing many redundant
genomes; for example, in Table @tbl:genbank-cover, we show that for
one mock and one real community, we select minimum metagenome covers of 19 and 99
genomes for metagenomes that contain matches to 406k and 96k GenBank
genomes total.

The min-set-cov approach for assigning genomes to metagenomes using
k-mers differs substantially from extant k-mer and mapping-based
approaches for identifying relevant genomes.  LCA-based approaches
such as Kraken label individual k-mers based on taxonomic lineages in
a database, and then use the resulting database of annotated k-mers to
assign taxonomy to reads. Mapping- and homology-based approaches such
as Diamond use read mapping to genomes or read alignment to gene
sequences in order to assign taxonomy and function
[@doi:10.1002/cpz1.59]. These approaches typically focus on assigning
*individual* k-mers or reads.  In contrast, here we analyze the entire
collection of k-mers and assign them _in aggregate_ to the _best_
genome match, and then repeat until no matches remain.

The resulting minimum metagenome cover can then be used as part of further
analyses, including both taxonomic content analysis and read mapping.
For taxonomic analyses, we find that this approach is competitive with
other current approaches and has several additional conveniences
(discussed in detail below).  The comparison of hash-based estimation
of containment to mapping results in Figure @fig:mapping suggests that
this approach is an accurate proxy for systematic mapping, as also seen
in Metalign [@metalign].

There is one significant drawback to assigning minimum metagenome
covers based on k-mers: because k-mers are not a perfect proxy for
mapping (e.g. see Figure @fig:mapping, blue lines), using k-mers to
identify the best genome for *mapping* may sometimes lead to
inaccurate assignments. Note that long k-mers are generally more
stringent and specific than mapping, so e.g. 51-mer overlaps can be
used to identify *some* candidate genomes for mapping, but not *all* candidate
genomes will necessarily be found using 51-mer overlaps.  The extent and impact of
this kind of false negative in the min-set-cov approach remains to be
evaluated but is likely to only affect strain- and species-level
assignments, since nucleotide similarity measures lose sensitivity
across more distant taxonomic ranks [@metapalette].

Our implementation of the min-set-cov algorithm in sourmash also
readily supports using custom reference databases as well as updating
minimum metagenome covers with the addition of new reference genomes. When
updating metagenome covers with new reference genomes, the first stage of
calculating overlaps can be updated with the new genomes (column 2 of
Table @tbl:genbank-cover), while the actual calculation of a *minimum*
set cover must be redone each time.

Minimum set cover approaches may provide opportunities beyond those
discussed here. For example, read- and contig-based analyses, and analysis
and presentation of alignments, can be potentially simplified with this
approach.

## Minimum metagenome covers support accurate and flexible taxonomic assignment

We can build a taxonomic classifier on top of minimum metagenome
covers by reporting the taxonomies of the constituent genomes,
weighted by distinct overlap and aggregated at the relevant taxonomic
levels.  Our CAMI-based taxonomic benchmarking
shows that this approach is competitive with several extant approaches for all
metrics across all taxonomic levels (Figures @fig:spider and
@fig:scores). This taxonomic accuracy also suggests that minimum metagenome covers
themselves are likely to be accurate, since the taxonomic assignment is
built solely on the metagenome cover.

One convenient feature of this approach to taxonomic analysis is that
new or changed taxonomies can be readily incorporated by assigning
them directly to genome identifiers; the majority of the computational
work here is involved in finding the reference genomes, which can have
assignments in multiple taxonomic frameworks. For example, sourmash
already supports GTDB [@doi:10.1093/nar/gkab776] natively, and will
also support the emerging LINS framework
[@doi:10.1094/PHYTO-07-16-0252-R].  sourmash can also readily
incorporate updates to taxonomies, e.g. the frequent updates to the
NCBI taxonomy, without requiring expensive reanalysis of the primary
metagenome data or even regenerating the minimum metagenome cover.

Interestingly, this framing of taxonomic classification as a minimum
set cover problem may also avoid the loss of taxonomic resolution that
affects k-mer- and read-based approaches on large databases
[@doi:10.1186/s13059-018-1554-6]; this is because we incorporate taxonomy
_after_ reads and k-mers have been assigned to individual genomes, and
choose entire *genomes* based on a greedy best-match-first approach.
This minimizes the impact of individual k-mers that may be common to
a genus or family, or were mis-assigned as a result of contamination.

Finally, as the underlying min-set-cov implementation supports custom
databases, it is straightforward to support taxonomic analysis using
*custom* databases and/or custom taxonomic assignments. This is
potentially useful for projects that are generating many new genomes
and wish to use them for metagenome analysis.  sourmash natively
supports this functionality.

Our current implementation of taxonomic assignment in sourmash does not
provide read-level assignment. However, it is a straightforward (if
computationally expensive) exercise to use the read mapping approach
developed in this paper to provide read-level taxonomic assignment along
with genome abundance estimation.

<!--
## Simple algorithms support performant implementations

The algorithms underlying both _Scaled MinHash_ and the greedy
min-set-cov approximation are simple to describe and straightforward
to implement.  This increases the likelihood of correct
implementation, provides opportunities for independent optimization of
data structures, and simplifies interoperability between different
implementations.

In the sourmash software package, we provide a mature and optimized
implementation that implements all of the operations above.  sourmash
performs well in practice and supports a wide variety of use cases
(CTB: see performance in appendix, docs and tutorials at
sourmash.rtfd.io, and installation instructions for pip and conda).
The sourmash project also provides large scale databases for NCBI and
GTDB taxonomies.
-->

## The minimum set cover approach is reference dependent

The min-set-cov approach is reference-based, and hence is entirely
dependent on the reference database. This may present challenges: 
for example, in many cases the exact reference strains present in the
metagenome will not be present in the database. This manifests in two
ways - see Figure @fig:mapping. First, for real metagenomes, there is a systematic mismatch
between the hash content and the mapping content (green line), because
mapping software is more permissive in the face of variants than
k-mer-based exact matching. Moreover, many of the lower rank genomes
in the plot are from the same species but different *strains* as the
higher ranked genomes, suggesting that strain-specific portions of the
reference are being utilized for matching at lower ranks. In reality,
there will usually be a different mixture of strains in the metagenome
than is present in the reference database. Methods for updating references from
metagenome data sets may provide an opportunity for generating
metagenome-specific references [@spacegraphcats].

The approach presented here chooses arbitrarily between
matches with equivalent numbers of contained k-mers. There are specific
genomic circumstances where this approach could usefully be refined with
additional criteria. For example, if a phage genome is present in the
reference database, and is also present within one or more genomes in the
database, it may desirable to select the match with the highest
Jaccard *similarity* in order to choose the phage genome. This is
algorithmically straightforward to implement when desired.

In light of the strong reference dependence of the min-set-cov
approach together with the insensitivity of the FracMinHash technique,
it may be useful to explore alternate methods of summarizing the list
of overlapping genomes, that is, summarizing *all* the genomes in column 2 of
Table @tbl:genbank-cover. For example, a hierarchical approach could
be taken to first identify the full list of overlapping genomes using
FracMinHash at a low resolution, followed by a higher resolution
(but more resource intensive) approach to identify the best matching genomes.

## Opportunities for future improvement of min-set-cov

There are a number of immediate opportunities for future improvement of
the min-set-cov approach.

Implementing min-set-cov on top of FracMinHash means our approach may
incorrectly choose between very closely related genomes, 
because the set of subsampled hashes may not discriminate
between them.  Likewise, the potentially very large size of the sketches
may inhibit the application of this approach to very large metagenomes.

These limitations are not intrinsic to min-set-cov, however; 
any data structure supporting both the _containment_ $C(A, B) =
\frac{\vert A \cap B \vert }{\vert A \vert}$ and _remove elements_
operations can be used to implement the greedy approximation algorithm.
For example, a simple _set_
of the $k$-mer composition of the query supports element removal, and
calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the Counting
Quotient Filter [@doi:10.1145/3035918.3035963] can also be used, with
the benefit of reduced storage and memory usage.

In turn, this means that limitations of our current implementation,
such as insensitivity to small genomes when $s$ is approximately the
same as the genome size, may be readily solvable with other sketch types.

There are other opportunities for improving on these initial explorations.
The availability of abundance counts for each
element in the FracMinHash is not well explored, since the
process of _removing elements_ from the query does not use them.
This may be important for genomes with more repetitive content such as
eukaryotic genomes.
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@doi:10.1101/gr.210641.116].

<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
