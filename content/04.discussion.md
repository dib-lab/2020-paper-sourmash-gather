# Discussion

## Scaled MinHash provides efficient compositional queries for large data sets.

_Scaled MinHash_ is an implementation of ModHash that uses the bottom
hashing concept from MinHash: all elements in the set to be sketched
are hashed, and any hash below a certain fixed boundary value are
kept. This fixed boundary value is determined by the desired accuracy
for the sketch representation. Unlike MinHash, _Scaled MinHash_
supports containment analysis between sets of very different sizes,
and here we demonstrate that it can be used efficiently and
effectively for compositional analysis of shotgun metagenome data sets
with k-mers. In particular, _Scaled MinHash_ is competitive in
accuracy with extant MinHash-based techniques for containment
analysis, while also supporting Jaccard similarity.  Footnote: We note
that others have also applied the ModHash concept to genomic data;
see, for example, Durbin's "modimizer"
[@https://github.com/richarddurbin/modimizer].

Intuitively, Scaled MinHash performs a density sampling at a rate of 1
$k$-mer per $s$ distinct k-mers seen, where $s$ is the size of the
hash space divided by the boundary value used in creating the
sketch. This is a kind of lossy compression with a compression ratio
of $s$: that is, for typical values of $s$ used here ($s \approx
1000$), data sets are reduced in size by 1000-fold.

No hash is ever removed from a Scaled MinHash during
construction; while this means that sketches grow proportionally to
the number of distinct k-mers in the sampled data set, they also
support many operations without needing to revisit the original data
set. This is in contrast to MinHash, which requires auxiliary data
structures for many operations - most especially, containment
operations (cite CMash and mash screen).  Thus Scaled MinHash
sketches serve as distributed compressed indices for the original
content for a much wider range of operations.

Because _Scaled MinHash_ sketches collect all hash values below a
fixed threshold, they support streaming analysis of sketches: any
operations that used a previously selected value can be cached and
updated with newly arriving values.  ModHash has similar
properties, but this is not the case for MinHash, since
after $n$ values are selected any displacement caused by new data can
invalidate previous calculations.

Scaled MinHash also directly supports the addition and subtraction of
hash values from a sketch, allowing post-processing and filtering without
revisiting the original data set. This includes unions and intersections.
Although possible for MinHash, in practice this requires
oversampling (using a larger $n$) to account for possibly having less
than $n$ values after filtering (the approach taken by Finch
[@bovee_finch:_2018]).

Another useful operation available on _Scaled MinHash_ is
*downsampling*: the contiguous value range for Scaled MinHash sketches
allow deriving $\mathbf{SCALED}_{s'}(W)$ sketches for any $s' \ge s$
using only $\mathbf{SCALED}_{s}(W)$.  MinHash and ModHash can also
support this operation in limited circumstances, as long as $n' \le n$
and $m'$ is a multiple of $m$.  Note also that Scaled MinHash and
regular MinHash sketches can be converted between each other when the same
hashing scheme is used, and when (insert math here about boundary
values etc.)

Abundance filtering is another extension to MinHash sketches, keeping
a count of how many times a value appears in the original data.  This
allows removing low-abundance values, as implemented in Finch
[@bovee_finch:_2018], another MinHash sketching software for genomics.
Filtering values that only appear once was implemented in
Mash by using a Bloom Filter and only adding values after they were
seen once, with later versions also implementing an extra counter
array to keep track of counts for each value in the MinHash.  These
operations can be done in Scaled MinHash without auxiliary data
structures.

In exchange, _Scaled MinHash_ sketches have limited sensitivity for
small data sets (data set size approx. $s$) and are only bounded in
size by H/s, which is typically quite large.  This limited sensitivity
may affect the sensitivity of gene- and viral genome-sized queries,
but at $s=1000$ we see comparable accuracy and sketch size
[qto MinHash for bacterial genome comparisons.

(CTB: maybe remove below:)

The consistency of operating in the same data structure also allows
further methods to be develop using only _Scaled MinHash_ sketches and
their features, especially if large collections of _Scaled MinHash_
sketches are available.  Because Scaled MinHash are collections of
hashes, existing k-mer indexing approaches can be applied to the
sketches to provide fast database search of these indices.

## min-set-cov supports accurate compositional analysis of metagenomes.

Many metagenome content analysis approaches use reference genomes to
interpret metagenome content.  Here, we frame the computational
challenge of discovering the appropriate reference genomes for a set
of metagenome reads as a min-set-cov problem, in which we seek the
*minimum* set of reference genomes necessary to account for all
mappable reads. We show that this can be resolved efficiently for
real-world data sets using a greedy algorithm together with _Scaled
MinHash_ and large-scale containment search of GenBank.

Our comparison of hash-based estimation of containment to mapping
results in Figure @fig:gather shows that this approach is an accurate
proxy for systematic mapping.  In particular, hash-based estimation of
containment closely matches actual read mapping performance.

Unlike Kraken-type approaches, min-set-cov analysis is not tied to
taxonomic assignment of genomes; this leads to both computational
efficiency in making downstream taxonomic assignments (see discussion below)
as well as providing robustness in the face of changing taxonomy.

Moreover, the development of a parsimonious list of relevant genomes
is convenient in the age of large reference databases with many
redundant genomes.

CTB: How does incompleteness in genome databases affect things? Might
be a way to connect with low-rank genome fuzz, and sgc.

The greedy algorithm used to determine the minimal list of genomes
also lends itself to incremental update with new genomes and supports
the use of private databases.

CTB/NTP: this is database dependent - say it straight up.
One confounding factor is that for real metagenomes, exact reference
strains are not usually present in the database. This manifests in two
ways in Figure @fig:minhash. First, there is a systematic mismatch
between the hash content and the mapping content (green line), because
mapping software is more permissive in the face of small variants than
k-mer-based exact matching. Moreover, many of the lower rank genomes
in the plot are from the same species but different *strains* as the
higher ranked genomes, suggesting that strain-specific portions of the
reference are being utilized for matching at lower ranks. In reality,
there will usually be a different mixture of strains in the metagenome
than in the reference database. Approaches such as spacegraphcats may
help resolve this by building new references, yada.

Mention weighted cover cc David?

Leftover text:

Our implementation of gather does not currently select the set of
smallest genomes, but rather the smallest set of genomes. If there are
two genomes with equal containment of the k-mers, it is arbitrary as
to which one is chosen.

Note that here we are providing one approach / approximation (Scaled
MinHash containment) with one shingling approach (k-mers) to tackle
metagenome composition for mapping and taxonomy. The min-set-cover
approach could be used with exact containment, and/or with other
shingling approaches.

xx can we guess at places where gather would break? One is equivalent
containment/different genome sizes.

Any data structure supporting both the _containment_ $C(A, B) =
\frac{\vert A \cap B \vert }{\vert A \vert}$ and _remove elements_
operations can be used as a query with `gather`.  For example, a _set_
of the $k$-mer composition of the query supports element removal, and
calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the _Counting
Quotient Filter_ [@pandey_general-purpose_2017] can also be used, with
the benefit of reduced storage and memory usage.  Moreover, the
collection of datasets can be implemented with any data structure that
can do containment comparisons with the query data structure.  Here it
can be important to have performant containment searches, since
`gather` may run `FindBestContainment` many times.

## min-set-cov supports accurate taxonomic classification of metagenome content

Once the min-set-cov approach has identified reference genomes, we can
build a taxonomic classifier for metagenome content by simply
reporting the taxonomies of the constitutent genomes.  Our initial
taxonomic benchmarking show that this approach is competitive for all
metrics across all taxonomic levels.

This approach does not result in the taxonomic saturation caused by
the increasing size of large reference databases associated with many
other k-mer based methods (Kraken, etc.). As long as every genome in
the database possesses a distinct combination of k-mers, the
min-set-cov approach can disambiguate reference genomes based on this
combination.  In practice, our use of _Scaled MinHash_ k-mer/hash
sampling will limit the resolution of our technique for very closely
related genomes, because distinct hashes will not be chosen for them.

One convenient feature of this approach to taxonomic analysis is that
new or changed taxonomies can be readily incorporated by assigning
them directly to genome identifiers; the majority of the compute is
involved in finding the reference genomes, which can have assignments
in different taxonomic frameworks. For example, sourmash already
supports GTDB natively, and will also support the emerging LINS
framework.  sourmash can also readily incorporate updates to
taxonomies, e.g. frequent updates to the NCBI taxonomy, without
requiring expensive reanalysis of the primary metagenome data or even
redoing the min-set-cov computation.

Finally, as with the underlying min-set-cov algorithm, it is
straightforward to support taxonomic analysis using custom databases
and/or custom taxonomic assignments; sourmash already supports this
natively.

## Algorithm is simple, computational performance is great

The algorithms underlying both _Scaled MinHash_ and the greedy
min-set-cov solution are simple to describe and straightforward to
implement.  This increases the likelihood of correct implementation,
provides opportunities for independent optimization of data structures,
and simplifies interoperability between different implementations.

We provide two implementations with this paper: sourmash, a fully
supported open source implementation with command-line, Python and
Rust APIs; and smol, a much shorter Rust implementation for
demonstration purposes.

## sourmash supports large scale data analysis

Taxonomic profiling is fundamentally limited by the availability of
reference datasets, even if new reference datasets can be derived from
clustering possible organisms based on sequence data in metagenomes
[@milanese_microbial_2019].  The sourmash project provides large scale
databases for NCBI and GTDB taxonomies, and supports search of all
available genomes.

### Limitations of gather

(For _Scaled MinHash_, `gather`, and taxonomy. Move where? Conclusions?)

`gather` as implemented in `sourmash` has the same limitations as
_Scaled MinHash_ sketches, including reduced sensitivity to small
genomes/sequences such as viruses.  _Scaled MinHash_ sketches don't
preserve information about individual sequences, and short sequences
using large scaled values have increasingly smaller chances of having
any of its $k$-mers (represented as hashes) contained in the sketch.
Because it favors the best containment, larger genomes are also more
likely to be chosen first due to their sketches have more elements,
and further improvements can take the size of the match in
consideration too.  Note that this is not necessarily the _similarity_
$J(A, B)$ (which takes the size of both $A$ and $B$), but a different
calculation that normalizes the containment considering the size of
the match.

`gather` is also a greedy algorithm, choosing the best containment
match at each step.  Situations where multiple matches are equally
well contained or many datasets are very similar to each other can
complicate this approach, and additional steps must be taken to
disambiguate matches.  The availability of abundance counts for each
element in the _Scaled MinHash_ is not well explored, since the
process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the
count in the match).
<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

(From David Koslicki)
Gotchas:

* Lack of sensitivity for small queries
* Potentially large sketch sizes

And a couple other that I’ve tentatively/mathematically observed:

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)
