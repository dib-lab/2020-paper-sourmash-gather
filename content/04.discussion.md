# Discussion

## Scaled MinHash provides efficient compositional queries for large data sets.

_Scaled MinHash_ is an implementation of ModHash using concepts from
MinHashing. _Scaled MinHash_ sketches support a variety of features
that are convenient for compositional queries, including containment,
hash removal, abundance tracking, and downsampling of sketches to
lower scaled values. (CTB: mention streaming, hash occurrence guarantees?)  In
exchange, _Scaled MinHash_ sketches have limited sensitivity for small
queries and are only bounded in size by H/s, which is usually quite
large.

Once a Scaled MinHash is calculated, the original data does not need
to be revisited during searches.  This allows sketches to serve as a
distributable compressed index for sequence content. Moreover, because
these sketches are collections of hashes, existing k-mer indexing
approaches can be applied to the sketches to provide fast database
search.

In exchange for these many conveniences, _Scaled MinHash_ sketches have
limited sensitivity on small data sets.  (More here.)
_Scaled MinHash_ sketches offer a fixed range of possible hash values,
but with reduced sensitivity for small datasets when using larger $s$
(scaled) values.  A biological example are viruses: at $s=2000$ many
viruses are too small to consistently have a hashed value selected by
the _Scaled MinHash_ approach.  Other _MinHash_ approaches sidestep
the problem by using hashing and streaming the query dataset (`Mash
Screen`) or loading the query dataset into an approximate query
membership data structure (`CMash`) to allow comparisons with the
variable range of possible hash values, but both solutions require the
original data or a more limited data representation than _Scaled
MinHash_.  The consistency of operating in the same data structure
also allows further methods to be develop using only _Scaled MinHash_
sketches and their features, especially if large collections of
_Scaled MinHash_ sketches are available.

Another drawback of Scaled MinHash when compared to regular MinHash sketches
is the size: the MinHash parameter $s$ sets an upper bound on the size
of the sketch, independently of the size of the original data.  Scaled
MinHash sketches grow proportionally to the original data cardinality,
and in the worst case can have up to $\frac{H}{s}$ items.

Intuitively, Scaled MinHash is performing a density sampling at a rate
of 1 $k$-mer per $s$ k-mers seen.

Others have also applied the ModHash concept to genomic data; see, for
example, Durbin's "modimizer"
[@https://github.com/richarddurbin/modimizer].

## Scaled MinHash sketches support many convenient operations.

Scaled MinHash supports many convenient operations that minimize
the need to reprocess the original data, which can be important for
genomics applications.

Because Scaled MinHash sketches collect any value below a threshold
this also guarantees that once a value is selected it is never
discarded.  This is useful in streaming contexts: any operations that
used a previously selected value can be cached and updated with new
arriving values.  $\mathbf{MOD}_m(W)$ has similar properties, but this
is not the case for $\mathbf{MIN}_n(W)$, since after $n$ values are
selected any displacement caused by new data can invalidate previous
calculations.

Scaled MinHash also directly supports the addition and subtraction of
hash values from a sketch, allowing post-processing and filtering.
Although possible for $\mathbf{MIN}_n(W)$, in practice this requires
oversampling (using a larger $n$) to account for possibly having less
than $n$ values after filtering (the approach taken by Finch
[@bovee_finch:_2018]).

Another useful operation is *downsampling*: the contiguous
value range for Scaled MinHash sketches allow deriving
$\mathbf{SCALED}_{s'}(W)$ sketches for any $s' \ge s$ using only
$\mathbf{SCALED}_{s}(W)$.  MinHash and ModHash can also support this
operation, as long as $n' \le n$ and $m'$ is a multiple of $m$.
Note also that Scaled MinHash and regular MinHash can be converted
between each other in certain situations.

Abundance filtering is another extension to MinHash sketches, keeping
a count of how many times a value appeared in the original data.  This
allows filtering for low-abundance values, as implemented in Finch
[@bovee_finch:_2018], another MinHash sketching software for genomics.
Filtering values that only appeared once was implemented before in
Mash by using a Bloom Filter and only adding values after they were
seen once, with later versions also implementing an extra counter
array to keep track of counts for each value in the MinHash.  These
operations can be done in Scaled MinHash without auxiliary data
structures.

(Mention differential privacy?)

## min-set-cov supports accurate compositional analysis of metagenomes.

Many metagenome content analysis approaches use reference genomes to
interpret metagenome content.  Here, we frame the computational
challenge of discovering the appropriate reference genomes for a set
of metagenome reads as a min-set-cov problem, in which we seek the
*minimum* set of reference genomes necessary to account for all
mappable reads. We show that this can be resolved efficiently for
real-world data sets using a greedy algorithm together with _Scaled
MinHash_ and large-scale containment search of GenBank.

Our comparison of hash-based estimation of containment to mapping
results in Figure @fig:gather shows that this approach is an accurate
proxy for systematic mapping.  In particular, hash-based estimation of
containment closely matches actual read mapping performance.

Unlike Kraken-type approaches, min-set-cov analysis is not tied to
taxonomic assignment of genomes; this leads to both computational
efficiency in making downstream taxonomic assignments (see discussion below)
as well as providing robustness in the face of changing taxonomy.

Moreover, the development of a parsimonious list of relevant genomes
is convenient in the age of large reference databases with many
redundant genomes.

CTB: How does incompleteness in genome databases affect things? Might
be a way to connect with low-rank genome fuzz, and sgc.

The greedy algorithm used to determine the minimal list of genomes
also lends itself to incremental update with new genomes and supports
the use of private databases.

One confounding factor is that for real metagenomes, exact reference
strains are not usually present in the database. This manifests in two
ways in Figure @fig:minhash. First, there is a systematic mismatch
between the hash content and the mapping content (green line), because
mapping software is more permissive in the face of small variants than
k-mer-based exact matching. Moreover, many of the lower rank genomes
in the plot are from the same species but different *strains* as the
higher ranked genomes, suggesting that strain-specific portions of the
reference are being utilized for matching at lower ranks. In reality,
there will usually be a different mixture of strains in the metagenome
than in the reference database. Approaches such as spacegraphcats may
help resolve this by building new references, yada.

Mention weighted cover cc David?

Leftover text:

Our implementation of gather does not currently select the set of
smallest genomes, but rather the smallest set of genomes. If there are
two genomes with equal containment of the k-mers, it is arbitrary as
to which one is chosen.

Note that here we are providing one approach / approximation (Scaled
MinHash containment) with one shingling approach (k-mers) to tackle
metagenome composition for mapping and taxonomy. The min-set-cover
approach could be used with exact containment, and/or with other
shingling approaches.

xx can we guess at places where gather would break? One is equivalent
containment/different genome sizes.

Any data structure supporting both the _containment_ $C(A, B) =
\frac{\vert A \cap B \vert }{\vert A \vert}$ and _remove elements_
operations can be used as a query with `gather`.  For example, a _set_
of the $k$-mer composition of the query supports element removal, and
calculating containment can be done with regular set operations.
Approximate membership query (AMQ) sketches like the _Counting
Quotient Filter_ [@pandey_general-purpose_2017] can also be used, with
the benefit of reduced storage and memory usage.  Moreover, the
collection of datasets can be implemented with any data structure that
can do containment comparisons with the query data structure.  Here it
can be important to have performant containment searches, since
`gather` may run `FindBestContainment` many times.

## min-set-cov supports accurate taxonomic classification of metagenome content

Once the min-set-cov approach has identified reference genomes, we can
build a taxonomic classifier for metagenome content by simply
reporting the taxonomies of the constitutent genomes.  Our initial
taxonomic benchmarking show that this approach is competitive for all
metrics across all taxonomic levels.

This approach does not result in the taxonomic saturation caused by
the increasing size of large reference databases associated with many
other k-mer based methods (Kraken, etc.). As long as every genome in
the database possesses a distinct combination of k-mers, the
min-set-cov approach can disambiguate reference genomes based on this
combination.  In practice, our use of _Scaled MinHash_ k-mer/hash
sampling will limit the resolution of our technique for very closely
related genomes, because distinct hashes will not be chosen for them.

One convenient feature of this approach to taxonomic analysis is that
new or changed taxonomies can be readily incorporated by assigning
them directly to genome identifiers; the majority of the compute is
involved in finding the reference genomes, which can have assignments
in different taxonomic frameworks. For example, sourmash already
supports GTDB natively, and will also support the emerging LINS
framework.  sourmash can also readily incorporate updates to
taxonomies, e.g. frequent updates to the NCBI taxonomy, without
requiring expensive reanalysis of the primary metagenome data or even
redoing the min-set-cov computation.

Finally, as with the underlying min-set-cov algorithm, it is
straightforward to support taxonomic analysis using custom databases
and/or custom taxonomic assignments; sourmash already supports this
natively.

## Algorithm is simple, computational performance is great

The algorithms underlying both _Scaled MinHash_ and the greedy
min-set-cov solution are simple to describe and straightforward to
implement.  This increases the likelihood of correct implementation,
provides opportunities for independent optimization of data structures,
and simplifies interoperability between different implementations.

We provide two implementations with this paper: sourmash, a fully
supported open source implementation with command-line, Python and
Rust APIs; and smol, a much shorter Rust implementation for
demonstration purposes.

## sourmash supports large scale data analysis

Taxonomic profiling is fundamentally limited by the availability of
reference datasets, even if new reference datasets can be derived from
clustering possible organisms based on sequence data in metagenomes
[@milanese_microbial_2019].  The sourmash project provides large scale
databases for NCBI and GTDB taxonomies, and supports search of all
available genomes.

### Limitations of gather

(For _Scaled MinHash_, `gather`, and taxonomy. Move where? Conclusions?)

`gather` as implemented in `sourmash` has the same limitations as
_Scaled MinHash_ sketches, including reduced sensitivity to small
genomes/sequences such as viruses.  _Scaled MinHash_ sketches don't
preserve information about individual sequences, and short sequences
using large scaled values have increasingly smaller chances of having
any of its $k$-mers (represented as hashes) contained in the sketch.
Because it favors the best containment, larger genomes are also more
likely to be chosen first due to their sketches have more elements,
and further improvements can take the size of the match in
consideration too.  Note that this is not necessarily the _similarity_
$J(A, B)$ (which takes the size of both $A$ and $B$), but a different
calculation that normalizes the containment considering the size of
the match.

`gather` is also a greedy algorithm, choosing the best containment
match at each step.  Situations where multiple matches are equally
well contained or many datasets are very similar to each other can
complicate this approach, and additional steps must be taken to
disambiguate matches.  The availability of abundance counts for each
element in the _Scaled MinHash_ is not well explored, since the
process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the
count in the match).
<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

(From David Koslicki)
Gotchas:

* Lack of sensitivity for small queries
* Potentially large sketch sizes

And a couple other that I’ve tentatively/mathematically observed:

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)
