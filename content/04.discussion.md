# Discussion

## Scaled MinHash provides efficient compositional queries for large data sets.

_Scaled MinHash_ is an implementation of ModHash using concepts from
MinHashing. _Scaled MinHash_ sketches support a variety of features
that are convenient for compositional queries, including containment,
hash removal, abundance tracking, and downsampling of sketches to
lower scaled values. (CTB: mention streaming, hash occurrence guarantees?)  In
exchange, _Scaled MinHash_ sketches have limited sensitivity for small
queries and are only bounded in size by H/s, which is usually quite
large -- practically speaking, _Scaled MinHash_ sketches are
unbounded wtih the input size.

Once a Scaled MinHash is calculated there are many operations that can
be applied without depending on the original data, thus serving as a
compressed index into content.

In exchange for these many conveniences, _Scaled MinHash_ sketches have
limited sensitivity on small data sets.  (More here.)

## min-set-cov supports accurate compositional analysis of metagenomes.

Many metagenome content analysis questions use the reference genoems
present in the metagenome as proxies for the metagenome content
itself.  Above, we frame the computational challenge of discovering
the appropriate reference genomes as a min-set-cov problem, in which
we seek the *minimum* set of reference genomes necessary to account
for mappable reads. We further show that this can be resolved
efficiently for real-world data sets using a greedy algorithm together
with _Scaled MinHash_ and large-scale containment search of GenBank.

Our comparison of hash-based estimation of containment to mapping
results in Figure @fig:gather shows that this approach is reasonably
accurate proxy for systematic mapping.  In particular, hash-based
estimation of containment closely matches actual mapping.

One confounding factor is that for real metagenomes, exact reference
strains are not usually present in the database. In Figure
@fig:minhash, this manifests in two ways. First, there is a systematic
mismatch between the hash content and the mapping content, because
mapping software is more permissive in the face of small variants than
k-mer matching, which relies on exact matches. Moreover, many of the
genomes to the right in the plot are from the same species but different
*strains* as the genomes to the left, suggesting that strain-specific
portions of the reference are being utilized for matching. In reality,
there will usually be a different mixture of strains in the metagenome
than in the reference database. Approaches such as spacegraphcats may
help resolve this by building new references, yada.

Note: gather can also be applied to private databases.

Leftover text:

Our implementation of gather does not currently select the set of
smallest genomes, but rather the smallest set of genomes. If there are
two genomes with equal containment of the k-mers, it is arbitrary as
to which one is chosen.

Note that here we are providing one approach / approximation (Scaled
MinHash containment) with one shingling approach (k-mers) to tackle
metagenome composition for mapping and taxonomy. The min-set-cover
approach could be used with exact containment, and/or with other
shingling approaches.

xx can we guess at places where gather would break? One is equivalent
containment/different genome sizes.

## min-set-cov supports accurate taxonomic classification of metagenome content

Once the min-set-cov approach has identified reference genomes, we can
build a taxonomic classifier for metagenome content by simply
reporting the taxonomies of the constitutent genomes.  Our initial
taxonomic benchmarking show that this approach is competitive for all
metrics across all taxonomic levels.

This approach avoids the taxonomic saturation problem for large
databases associated with many other k-mer based methods (Kraken,
etc.). As long as every genome in the database possesses a distinct
combination of k-mers, the min-set-cov approach can disambiguate
reference genomes based on this combination.  In practice, our use of
_Scaled MinHash_ k-mer/hash sampling will limit the resolution of our
technique for very closely related genomes, because distinct hashes
will not be chosen for them.

One convenient feature of this approach to taxonomic analysis is that
new or changed taxonomies can be readily incorporated by assigning
them directly to genome identifiers; the majority of the compute is
involved in finding the reference genomes, which can then be given
assignments in different taxonomic frameworks as needed. For example,
sourmash already supports GTDB natively, and will also support the
emerging LINS framework.  sourmash can also readily incorporate the
frequent updates to the NCBI taxonomy without requiring expensive
reanalysis of the primary metagenome data or even redoing the
min-set-cov computation.

Finally, as with the underlying min-set-cov algorithm, it is
straightforward to support taxonomic analysis using custom databases
and/or custom taxonomies.

## Algorithm is simple, computational performance is great

sourmash software is awesome.

### Gather can be applied to all the data.

Performant implementation in sourmasha Python API for data exploration
and methods prototyping.

<!-- Scaling to large collections of references --> Taxonomic
profiling is fundamentally limited by the availability of reference
datasets, even if new reference datasets can be derived from
clustering possible organisms based on sequence data in metagenomes
[@milanese_microbial_2019].  `gather` as implemented in `sourmash` is
a method that can scale to increasingly larger collections of datasets
due to multiple reasons:

  - containment and similarity estimation with _Scaled MinHash_
    sketches has lower computational requirements than alignment over
    all reads of a dataset;

  - since _Scaled MinHash_ sketches use a subset of the $k$-mer
    composition, they also scale better than full $k$-mer composition
    representations, requiring less space and reducing the number of
    elements to be computed;

  - querying multiple databases can be done independently, avoiding
    the need to merge, update or reprocess databases when new datasets
    are available.  A new database with the new datasets can be
    constructed and queried together with previous ones.

### Limitations of gather

(For _Scaled MinHash_, `gather`, and taxonomy. Move where? Conclusions?)

`gather` as implemented in `sourmash` has the same limitations as
_Scaled MinHash_ sketches, including reduced sensitivity to small
genomes/sequences such as viruses.  _Scaled MinHash_ sketches don't
preserve information about individual sequences, and short sequences
using large scaled values have increasingly smaller chances of having
any of its $k$-mers (represented as hashes) contained in the sketch.
Because it favors the best containment, larger genomes are also more
likely to be chosen first due to their sketches have more elements,
and further improvements can take the size of the match in
consideration too.  Note that this is not necessarily the _similarity_
$J(A, B)$ (which takes the size of both $A$ and $B$), but a different
calculation that normalizes the containment considering the size of
the match.

`gather` is also a greedy algorithm, choosing the best containment
match at each step.  Situations where multiple matches are equally
well contained or many datasets are very similar to each other can
complicate this approach, and additional steps must be taken to
disambiguate matches.  The availability of abundance counts for each
element in the _Scaled MinHash_ is not well explored, since the
process of _removing elements_ from the query doesn't account for them
(the element is removed even if the count is much higher than the
count in the match).
<!-- David comment: could use a compressive sensing approach here:
$ min \norm{x}^2_1 + \lambda \norm{Ax - y}^2_2, x \ge 0$
Y_i = count of hash i in sample
A_ij = count of hash i in genome j
convert to least squares and use Lawson and Hanson for blistering speed!
-->
Both the multiple match as well as the abundance counts issues can benefit from
existing solutions taken by other methods,
like the _species score_ (for disambiguation) and _Expectation-Maximization_ (for abundance analysis)
approaches from Centrifuge [@kim_centrifuge_2016].

(From David Koslicki)
Gotchas:

* Lack of sensitivity for small queries
* Potentially large sketch sizes

And a couple other that I’ve tentatively/mathematically observed:

* The variance of the estimate of C(A,B)=|A\cap B| / |A| appears to
  also depend on |A\B|, which was somewhat surprising
* The “fixed k-size” problem (which might be able to be overcome with
  the prefix-lookup data structure, if one sacrifices some accuracy)
