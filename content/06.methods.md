# Methods

## Implementation of FracMinHash and min-set-cov

We provide implementations of FracMinHash and min-set-cov in the
software package `sourmash`, which is implemented in Python and Rust
and developed under the BSD license [@doi:10.21105/joss.00027] (cite
joss, zenodo latest version, github URL). FracMinHash sketches are
created for DNA sequence inputs using the `sourmash sketch dna`
command with the `scaled` parameter. Minimum metagenome covers are
generated using `sourmash gather` with the sketched metagenome as
query against a collection of one or more sketched genomes.

## Comparison between CMash, mash screen, and Scaled MinHash.

Experiments use $k=\{21, 31, 51\}$ (except for Mash, which only
supports $k \le 32$).  For Mash and CMash they were run with
$n=\{1000, 10000\}$ to evaluate the containment estimates when using
larger sketches with sizes comparable to the FracMinHash sketches
with $scaled=1000$.  The truth set is calculated using an exact
$k$-mer counter implemented with a _HashSet_ data structure in the
Rust programming language [@matsakis_rust_2014].

For _Mash Screen_ the ratio of hashes matched by total hashes is used
instead of the _Containment Score_, since the latter uses a $k$-mer
survival process modeled as a Poisson process first introduced in
[@fan_assembly_2015] and later used in the _Mash distance_
[@ondov_mash:_2016] and _Containment score_ [@ondov_mash_2019]
formulations.

## GenBank database sketching and searches

Minimum metagenome covers were calculated using a microbial genome
subset of GenBank (date XYZ, number of genomes ZZZ) using a scaled
factor of 2000 and a k-mer size of 31. Sketches for all genomes and
metagenomes were calculated with `sourmash sketch dna -p
scaled=2000,k=31`. The minimum metagenome covers were calculated using
all genomes sharing 100 hashes with the metagenome (that is, an
estimated overlap of 100,000 k-mers) with
`sourmash gather --threshold-bp 1e5`.
Overlapping sketches were saved with `--save-prefetch`
and matches were saved with `--save-matches`.

The GenBank database used is XYZ GB in size and is available for download
at ZZZ.

## Taxonomy

## Genome retrieval and mapping workflow

mapping

The complete workflow, from metagenome download to taxonomic analysis
and iterative mapping, is implemented in the genome-grist package
(version, doi, etc.). genome-grist uses snakemake (cite) to implement
a workflow that combines sourmash sketching, metagenome cover
calculation, and taxonomic analysis with metagenome download from the
SRA, genome download from GenBank, and read mapping.

The summary results from genome-grist for this paper are available HERE.

## Figures and notebooks for this paper.

## Revised theoretical analysis of FracMinHash

{% raw %}
```{=latex}
\def\scaleb{\hat{C}_\text{frac}(A,B)}
```
{% endraw %}

Given two arbitrary sets $A$ and $B$ which are subsets of a domain
$\Omega$, the containment index $C(A,B)$ is defined as
$C(A,B):=\frac{\vert A \cap B \vert}{\vert A \vert}$. Let $h$ be a
perfect hash function $h~:~\Omega \rightarrow~[0,H]$ for some $H\in
\mathbb{R}$. For a *scale factor* $s$ where $0 \le s \le 1$, a
FracMinHash sketch of a set $A$ is defined as follows:

{% raw %}
```{=latex}
\begin{equation}
    %\mathbf{FRAC}_S(A) = \left\{\,h(a) \mid \forall a \in A\ {\rm s.t.}\ h(a) \leq Hs\right\}.
    \mathbf{FRAC}_S(A) = \left\{\,h(a) \mid \forall a \in A\ {\rm s.t.}\ h(a) \leq Hs\right\}.
\end{equation}
```
{% endraw %}

The scale factor $s$ is a tunable parameter that can modify the size of the sketch. Using this FracMinHash sketch, we define the FracMinHash estimate of the containment index $\hat{C}_\text{frac}(A,B)$ as follows:

{% raw %}
```{=latex}
\begin{equation}
    \hat{C}_\text{scale}(A,B):=\frac{\vert \mathbf{FRAC}_S(A) \cap \mathbf{FRAC}_S(B)\vert }{\vert \mathbf{FRAC}_S(A)\vert}.
\end{equation}
```
{% endraw %}

For notational simplicity, we define $X_A := \vert \mathbf{FRAC}_S(A)
\vert$. Observe that if one views $h$ as a uniformly distributed
random variable, we have that $X_A$ is distributed as a binomial
random variable: $X_A \sim {\rm Binom}(|A|, s)$. Furthermore, if
$A\cap B = \emptyset$ where both $A$ and $B$ are non-empty sets, then
$X_A$ and $X_B$ are independent when the probability of success is
strictly smaller than $1$. Using these notations, we compute the
expectation of (equation).


For $0<s<1$, if $A$ and $B$ are two distinct sets such that $A \cap B$ is non-empty,
{% raw %}
```{=latex}
\begin{align*}
\E\left[\scaleb \mathbbm{1}_{\vert \mathbf{FRAC}_S(A) \vert>0} \right] =
\frac{\vert A\cap B \vert}{\vert A \vert} \left(1-(1-s)^{\vert A\vert}\right).
\end{align*}
```
{% endraw %}


Using the notation introduced previously, observe that 
$$
\scaleb \mathbbm{1}_{\vert \mathbf{FRAC}_S(A) \vert>0} = \frac{\X}{\X + \Y} \mathbbm{1}_{\X + \Y>0},
$$

and that the random variables $\X$ and $\Y$ are independent (which follows directly from the fact that $A \cap B$ is non-empty, and because $A$ and $B$ are distinct, $A \setminus B$ is also non-empty).
We will use the following fact from standard calculus:

{% raw %}
```{=latex}
\begin{align}
    \int_0^1 x t^{x+y-1}\, dt = \frac{x}{x+y} \mathbbm{1}_{x+y>0}.
\end{align}
```
{% endraw %}

Then using the moment generating function of the binomial distribution, we have

{% raw %}
```{=latex}
\begin{align}
    \E\left[t^\X\right] &= (1-s+st)^{\vert A \cap B \vert}\\
    \E\left[t^\Y\right] &= (1-s+st)^{\vert A \setminus B \vert}.
\end{align}
We also know by continuity that 
\begin{align}
    \E\left[\X \, t^{\X-1}\right] &= \frac{d}{dt} (1-s+st)^{\vert A \cap B \vert}\\
    &= \vert A\cap B \vert s (1-s+st)^{\vert A\cap B\vert-1}.
\end{align}
```
{% endraw %}

Using these observations, we can then finally calculate that 


{% raw %}
```{=latex}
\begin{align}
    \E\left[\frac{\X}{\X + \Y} \mathbbm{1}_{\X + \Y>0},\right] &= \E\left[\int_0^1 \X \,  t^{\X+\Y-1}\,dt\right]\\
    &= \int_0^1 \E\left[\X  \, t^{\X+\Y-1}\,dt\right]\label{line:1}\\
    &= \int_0^1 \E\left[\X  \, t^{\X-1}\right] \E\left[t^\Y\right]\,dt\label{line:2}\\
    &= \vert A\cap B\vert \int_0^1(1-s+st)^{\vert A\cap B \vert + \vert A\setminus B \vert -1}\, dt\\
    &= \frac{\vert A \cap B\vert (1-s+st)^{\vert A \vert}}{\vert A \vert}\bigg\rvert_{t=0}^{t=1}\\
    &= \frac{\vert A\cap B \vert}{\vert A \vert} \left(1-(1-s)^{\vert A\vert}\right),
\end{align}
```
{% endraw %}

where Fubini's theorem is used in \Cref{line:1} and independence in \Cref{line:2}.


In light of (theorem), we note that \cref{eqn:scaleC} is *not* an unbiased estimate of $C(A,B)$. This may explain the observations in (Luiz thesis) that showed the uncorrected version in (eqn) leads to suboptimal performance for short sequences (e.g viruses). However, for sufficiently large $\vert A \vert$ and $s$, the bias factor $\left(1-(1-s)^{\vert A\vert}\right)$ is sufficiently close to 1.

## Theoretical analysis of Scaled MinHash

{% raw %}
```{=latex}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\DeclareMathOperator*{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\Cov}{\mathrm{Cov}}
\def\E{\mathrm{E}}
\def\X{{X_{A\cap B}}}
\def\Y{{X_{A\setminus B}}}
\newcommand\bigoh{{\mathcal O}\xspace}
\def\P{\mathrm{Pr}}
\def\Pr{\mathrm{Pr}}
\def\scaleb{\hat{C}_\text{scale}(A,B)}
\def\scale{C_\text{scale}(A,B)}
```
{% endraw %}

### Expectation

In this section, we aim to study how well
```{=latex}
\begin{align}
\scaleb:=\frac{\vert \mathbf{SCALED}_s(A) \cap \mathbf{SCALED}_s(B)\vert }{\vert \mathbf{SCALED}_s(A)\vert }
\label{eqn:scaleC}
\end{align}
```
approximates the containment index
```{=latex}
\begin{align}
\label{eqn:C}
C(A,B):=\frac{\vert A \cap B \vert}{\vert A \vert},
\end{align}
```
for two arbitrary sets $A,B$ which are subsets of the domain $\Omega$ of the uniform hash function $h:\Omega \rightarrow [0,H]$. By rescaling, we can assume without loss of generality that $H=1$ and $0<s\leq H$ in the definition
$$
\mathbf{SCALED}_s(A) = \left\{\,h(a) \mid \forall a \in A\ {\rm s.t.}\ h(a) \leq \frac{H}{s}\right\}.
$$

For notational simplicity, we define $X_A := \vert \mathbf{SCALED}_s(A) |$. Observe that if one views $h$ as a uniformly distributed random variable, we have that $X_A$ is distributed as a binomial random variable: ${\rm Binom}(|A|, s)$. Furthermore, if $A\cap B = \emptyset$, then $X_A$ and $X_B$ are independent. We then compute the expectation of \cref{eqn:scaleC}.

```{=latex}
\begin{theorem}
\label{thm:Escaled}
For $0<s\leq 1$,
\begin{align*}
\E\left[\scaleb \mathbbm{1}_{\vert \mathbf{SCALED}_s(A) \vert>0} \right] =
\frac{\vert A\cap B \vert}{\vert A \vert} \left(1-(1-s)^{\vert A\vert}\right)
\end{align*}
\end{theorem}
```

```{=latex}
\begin{proof}
Using the notation introduced previously, observe that
$$
\scaleb \mathbbm{1}_{\vert \mathbf{SCALED}_s(A) \vert>0} = \frac{\X}{\X + \Y} \mathbbm{1}_{\X + \Y>0},
$$
and that the random variables $\X$ and $\Y$ are independent.
We next collect a few useful facts: from standard calculus,
\begin{align}
    \int_0^1 x t^{x+y-1}\, dt = \frac{x}{x+y} \mathbbm{1}_{x+y>0}.
\end{align}
Then using the moment generating function of the binomial distribution, we have
\begin{align}
    \E\left[t^\X\right] &= (1-s+st)^{\vert A \cap B \vert}\\
    \E\left[t^\Y\right] &= (1-s+st)^{\vert A \setminus B \vert}.
\end{align}
We also know by continuity that
\begin{align}
    \E\left[\X \, t^{\X-1}\right] &= \frac{d}{dt} (1-s+st)^{\vert A \cap B \vert}\\
    &= \vert A\cap B \vert s (1-s+st)^{\vert A\cap B\vert-1}.
\end{align}
Using these observations, we can then finally calculate that
\begin{align}
    \E\left[\frac{\X}{\X + \Y} \mathbbm{1}_{\X + \Y>0},\right] &= \E\left[\int_0^1 \X \,  t^{\X+\Y-1}\,dt\right]\\
    &= \int_0^1 \E\left[\X  \, t^{\X+\Y-1}\,dt\right]\label{line:1}\\
    &= \int_0^1 \E\left[\X  \, t^{\X-1}\right] \E\left[t^\Y\right]\,dt\label{line:2}\\
    &= \vert A\cap B\vert s \int_0^1(1-s+st)^{\vert A\cap B \vert + \vert A\setminus B \vert -1}\, dt\\
    &= \frac{\vert A \cap B\vert (1-s+st)^{\vert A \vert}}{\vert A \vert}\bigg\rvert_{t=0}^{t=1}\\
    &= \frac{\vert A\cap B \vert}{\vert A \vert} \left(1-(1-s)^{\vert A\vert}\right),
\end{align}
where Fubini's theorem is used in \cref{line:1} and independence in \cref{line:2}.
\end{proof}
```

In light of \cref{thm:Escaled}, we note that \cref{eqn:scaleC} is not an unbiased estimate of \cref{eqn:C}, but for $\vert A \vert$ sufficiently large, the bias factor $\left(1-(1-s)^{\vert A\vert}\right)$ is sufficiently close to 1. Alternatively, if $|A|$ is known (or estimated, eg. by using HyperLogLog \cite{flajolet2007hyperloglog}), then
$$
\scale := \frac{\vert \mathbf{SCALED}_s(A) \cap \mathbf{SCALED}_s(B)\vert }{\vert \mathbf{SCALED}_s(A)\vert \left(1-(1-s)^{\vert A\vert}\right)} \mathbbm{1}_{\vert \mathbf{SCALED}_s(A) \vert>0}
$$
is an unbiased estimate of the containment index.

### Variance
We can calculate the variance of $\scale$ directly from the associated multivariate probability mass function. Indeed, if we let $n=|A\cap B|$ and $m=|A\setminus B|$, since $X_{A\cap B}\sim {\rm Binom}(n, s)$ and $X_{A\setminus B}\sim {\rm Binom}(m, s)$ are independent, the probability mass function is:
```{=latex}
\begin{align*}
f_{C_\text{scale}}(k, l) = \binom{n}{k} \binom{m}{l} s^{p+k} (1-s)^{m+n-k-l}\quad {\rm for}\ k=0,\dots, n,\ {\rm and}\ l=0,\dots, m.
\end{align*}
```
The variance of the unbiased estimator is then computed as:
```{=latex}
\begin{align}
    \Var\left[\scale \right] &= \E\left[\scale\right]^2 - \E\left[\scale^2\right]\\
    &= \frac{|A\cap B|^2}{|A|^2} - \left(1-(1-s)^{ \vert A\vert}\right)^{-2} \sum_{k=1}^n \sum_{l=0}^m \frac{k^2}{(k+l)^2} \binom{n}{k} \binom{m}{l} s^{k+l} (1-s)^{n+m-k-l} \label{eqn:double}
\end{align}
```
Unfortunately, the double sum in \cref{eqn:double} appears to have no closed form representation. However, in practice, it is easily computed numerically when given particular values of $m$, $n$ and $s$. A first order Taylor series approximation of the variance can be computed.

```{=latex}
\begin{theorem}
For $n=|A\cap B|$ and $|A\setminus B|=m$, a first order Taylor series approximation gives
\begin{align*}
\Var\left[\scale \right] \approx \frac{mn(1-s)}{s(m+n)^3}.
\end{align*}
\end{theorem}
\begin{proof}
Let $g(x,y)=\frac{x}{x+y}$, $\mu_x = ns$, $\mu_y = ms$ and use subscripts to denote partial derivatives:
\begin{align*}
    g_x(x,y) &= \frac{y}{(x+y)^2}\\
    g_y(x,y) &= \frac{-x}{(x+y)^2}
\end{align*}
We then have the first order Taylor series:
\begin{align}
    \Var\left(g\left(\X,\Y\right)\right) &= g_x^2(\mu_x,\mu_y) \Var(\X) + 2 g_x(\mu_x,\mu_y)g_y(\mu_x,\mu_y) \E[\X-\mu_x]\E[\Y-\mu_y] \notag \\
    &+ g_y^2(\mu_x,\mu_y)\Var(\Y) \label{eqn:zeroterm}\\
    &= \frac{m^2}{s^2(m+n)^4} ns(1-s) + \frac{n^2}{s^2(m+n)^4} ms(1-s) \notag \\
    &= \frac{mn(1-s)}{(m+n)^3s},\notag
\end{align}
with the middle term of \cref{eqn:zeroterm} factoring due to independence.
\end{proof}
```
Proceeding in the same fashion, we can obtain second and third order approximations to the variance. Indeed, series approximations can be had to arbitrarily high order due to the binomial distribution having finite central moments of arbitrary order.

```{=latex}
\begin{theorem}
For $n=|A\cap B|$ and $|A\setminus B|=m$, a second order Taylor series approximation gives
\begin{align*}
\Var\left[\scale \right] \approx \frac{m n (1-p) \left(p^2 \left(m^2+m (2 n+3)+n (n+3)+6\right)-p (m+n+6)+1\right)}{p^3 (m+n)^5}.
\end{align*}
\end{theorem}
```

```{=latex}
\begin{theorem}
For $n=|A\cap B|$ and $|A\setminus B|=m$, a third order Taylor series approximation gives
\begin{align*}
\Var\left[\scale \right] \approx& s^{-5} (m+n)^{-7} m n (1-s) \Big(s \Big(s^2 (m+n-4) \Big(m^2+2 m (n+2)+n (n+4)+60\Big)\\
&+ s^3 \Big(m^4+m^3 (4 n+1)+m^2 \Big(6 n^2+3 n+5\Big)+m (n (n (4 n+3)+10)-10)\\
&+ n \Big(n\Big(n^2+n+5\Big)-10\Big)+120\Big)\\
&- s (m+n) (2 m+2 n+41)+9 m+9 n+150 s-30\Big)+1\Big).
\end{align*}
\end{theorem}
```

### Asymptotic normality

To show asymptotic normality of $\scale$, we utilize the delta method \cite[section 14.1.3]{agresti2003categorical} combined with the De Moivre-Laplace theorem. Indeed, since

#### Confidence intervals and hypothesis tests

Recall that an {\em approximate $(1-\alpha)$-confidence interval} of a distribution parameter $s$, is an interval which contains $s$ with limiting probability $1 - \alpha$.
Closely related, an {\em approximate hypothesis test with significance level $(1-\alpha)$} is an interval
that contains a limiting random variable with probability $1-\alpha$.
We will omit the word ``approximate'' in the rest of the paper, for brevity and also use the notation $X \in x \pm y$ to mean $X \in [x-y,x+y]$.
Finally, for $0 < \alpha < 1$, we let $z_\alpha = \Phi^{-1}(1- \alpha/2)$,
where $\mathrm{\Phi}^{-1}$ is the inverse of the cumulative distribution function of the standard normal distribution.

#### Other stuff

This section is a collection of other observations that might be useful.

First, there is a lot of theory about ratios of independent random variables. Here, however, we have a ratio of correlated random variables, but we do know their covariance.

```{=latex}
\begin{proposition}
$\Cov(\X,\X+\Y) = ns(1-s)$.
\end{proposition}
\begin{proof}
\begin{align*}
    \Cov(\X,\X+\Y) &= \E[\X(\X+\Y)] - \E[\X]\E[\X+\Y] \\
    &= \E[\X^2] + \E[\X\Y]-\E[\X]\E[\X+\Y]\\
    &= ns(1+(n-1)s) + n m s^2 - ns(ns + ms)\\
    &= ns(1-s).
\end{align*}
\end{proof}
```
Interestingly, this means that the covariance doesn't depend on $\Y$ at all, but rather $\Cov(\X,\X+\Y)=\Cov(\X,\X)=\Var(\X)$.
