# Methods

## Implementation of Scaled MinHash

We provide two implementations of Scaled MinHash, `smol` and
`sourmash`.  `smol` is a minimal implementation of _Scaled MinHash_
developed to demonstrate the method; it does not include many required
features for working with real biological data, but its smaller code
base makes it a more readable and concise example of the method.
`sourmash` [@doi:10.21105/joss.00027] implements features and
functionality needed for large scale analyses of real data.

## Comparison between CMash, mash screen, and Scaled MinHash.

Experiments use $k=\{21, 31, 51\}$ (except for Mash, which only
supports $k \le 32$).  For Mash and CMash they were run with
$n=\{1000, 10000\}$ to evaluate the containment estimates when using
larger sketches with sizes comparable to the Scaled MinHash sketches
with $scaled=1000$.  The truth set is calculated using an exact
$k$-mer counter implemented with a _HashSet_ data structure in the
Rust programming language [@matsakis_rust_2014].

For _Mash Screen_ the ratio of hashes matched by total hashes is used
instead of the _Containment Score_, since the latter uses a $k$-mer
survival process modeled as a Poisson process first introduced in
[@fan_assembly_2015] and later used in the _Mash distance_
[@ondov_mash:_2016] and _Containment score_ [@ondov_mash_2019]
formulations.

## MHBT

The _MinHash Bloom Tree_ (_MHBT_) is a variation of the _Sequence
Bloom Tree_ (_SBT_) that uses Scaled MinHash sketches as leaf nodes
instead of Bloom Filters as in the SBT.  The search operation in SBTs
is defined as a breadth-first search starting at the root of the tree,
using a threshold of the original $k$-mers in the query to decide when
to prune the search.  MHBTs use a query Scaled MinHash sketch instead,
but keep the same search approach.  The threshold of a query $Q$
approach introduced in [@solomon_fast_2016] is equivalent to the
containment $$C(Q, S) = \frac{\vert Q \cap S \vert }{\vert S \vert}$$
described in [@broder_resemblance_1997], where $S$ is a Scaled MinHash
sketch.  For internal nodes $n$ (which are Bloom Filters) the
containment of the query Scaled MinHash sketch $Q$ is
$$
C(Q, n) = \frac{\vert \{\,h \in n \mid \forall h \in Q\,\} \vert}{\vert Q
   \vert}
$$
as defined by
[@koslicki_improving_2019] for the _Containment MinHash_ to _Bloom
Filter_ comparison.

MHBTs support both containment and similarity queries.
For internal nodes the containment $C(Q,n)$ is used as an upper-bound of the similarity $J(Q, n)$:
$$C(Q, n) &\ge J(Q, n) \\
  \frac{\vert Q \cap n \vert }{\vert Q \vert} \ge \frac{\vert Q \cap n \vert }{\vert Q \cup n \vert}
  $$
since $\vert Q \cup n \vert \ge \vert Q \vert$.
When a leaf node is reached then the similarity $J(Q, S)$ is calculated for the Scaled MinHash sketch $S$
and declared a match if it is above the threshold $t$.
Because the upper-bound is being used,
this can lead to extra nodes being checked,
but it simplifies implementation and provides better correctness guarantees.

## Inverted index

The LCA index in `sourmash` is an inverted index that stores a mapping
from hashes in a collection of signatures to a list of IDs for
signatures containing the hash.  Despite the name, the list of
signature IDs is not collapsed to the lowest common ancestor (as in
kraken), and is calculated as needed by downstream methods using
taxonomy information stored separately in the LCA index.

The mapping from hashes to signature IDs in the LCA index is an
implicit representation of the original signatures used to build the
index, and so returning the signatures is implemented by rebuilding
the original signatures on-the-fly.  Search in an LCA index matches
the $k$-mers in the query to the list of signatures IDs containing
them, using a counter data structure to sort results by number of
hashes per signature ID.  The rebuilt signatures are then returned as
matches based on the signature ID, with containment or similarity to
the query calculated against the rebuilt signatures.

mash screen [@ondov_mash_2019] has a similar index, but it is
constructed on-the-fly using the distinct hashes in a sketch
collection as keys, and values are counters initially set to zero.  As
the query is processed, matching hashes have their counts incremented,
and after all hashes in the query are processed then all the sketches
in the collection are checked in the counters to quantify the
containment/similarity of each sketch in the query.  The LCA index
uses the opposite approach, opting to reconstruct the sketches
on-the-fly.

