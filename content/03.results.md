# Results

## Scaled MinHash sketches support accurate containment operations

We define the Scaled MinHash on an input domain of $k$-mers, $W$, as follows:

$$\mathbf{SCALED}_s(W) = \{\,w \leq \frac{H}{s} \mid \forall w \in
W\,\}$$ where $H$ is the largest possible value in the domain of
$h(x)$ and $\frac{H}{s}$ is the \emph{maximum hash} value in the
Scaled MinHash.

The Scaled MinHash is a mix of MinHash and ModHash
[@doi:10.1109/SEQUEN.1997.666900].  It keeps the selection of the
smallest elements from MinHash, while using the dynamic size from
ModHash to allow containment estimation.  However, instead of taking
$0 \mod m$ elements like $\mathbf{MOD}_m(W)$, a Scaled MinHash uses the
parameter $s$ to select a subset of $W$.

Scaled MinHash supports containment estimation with high accuracy and
low bias. (Analytic work from David HERE.)

* approximation formula (eqn 13 from overleaf)
* for queries into large sets (large $|A|$), bias factor is low.
* refer to appendix for derivation.

Given a uniform hash function $h$ and $s=m$, the cardinalities of
$\mathbf{SCALED}_s(W)$ and $\mathbf{MOD}_m(W)$ converge for large
$\vert W \vert$.  The main difference is the range of possible values
in the hash space, since the Scaled MinHash range is contiguous and
the ModHash range is not.  This permits a variety of convenient
operations on the sketches, including iterative downsampling of Scaled
MinHash sketches as well as conversion to MinHash sketches.

## Scaled MinHash accurately estimates containment between sets of different sizes

We compare the _Scaled MinHash_ method to
CMash (_Containment MinHash_) and Mash Screen (_Containment Score_)
for containment queries in the Shakya dataset
[@doi:10.1111/1462-2920.12086 ], a synthetic mock metagenomic
bacterial and archaeal community where the reference genomes are
largely known.  This data set has been used in several methods evaluations
[@awad_evaluating_2017,@ondov_mash_2019]]. (CTB add SPADes etc refs.)

![
**Letter-value plot [@hofmann_letter-value_2017] of the
differences from containment estimate to ground truth (exact).**
Each method is evaluated for $k=\{21,31,51\}$,
except for `Mash` with $k=51$, which is unsupported.
**A**: Using all 68 reference genomes found in previous articles.
**B**: Excluding low coverage genomes identified in previous articles.
](images/containment.svg "Containment estimation between smol, CMash, and mash screen"){#fig:containment}

All methods are within 1\% of the exact containment on average (Figure
@fig:containment A), with `CMash` consistently underestimating
the containment for large $k$ and overestimating for small $k$.  `Mash
Screen` with $n=10000$ has the smallest difference to ground truth for
$k=\{21, 31\}$, followed by `smol` with `scaled=1000` and `Mash
Screen` with $n=1000$.

Figure @fig:containment B shows results with low-coverage and
contaminant genomes (as described in [@awad_evaluating_2017] and
[@ondov_mash_2019]) removed from the database.  The number of outliers
is greatly reduced, with most methods within 1\% absolute difference
to the ground truth.  `CMash` still has some outliers with up to 8\%
difference to the ground truth.

CTB questions:

* should we _just_ use (B) benchmark?
* should we add sketch sizes in here more explicitly? e.g. number of hashes kept?
* compares well with others
* How much is missed figure; Poisson calculations? => appendix?

## Reference genomes can be selected for a metagenome using a simple greedy algorithm

We next ask: what is the smallest collection of genomes in a database
that should be used as a reference for k-mer based analysis of a
metagenome?  This question can be framed formally as follows: for a
given metagenome $M$ and a reference database $D$, what is the minimal
collection of genomes in $D$ which contain all of the k-mers in the
intersection of $D$ and $M$? That is, we wish to find the smallest set
$\{ G_n \}$ of genomes in $D$ such that $$k(M) \cap k(D) = \bigcup_n
\{ k(M) \cap k(G_n) \} $$

This is the *minimal set covering* problem, for which
there is a polynomial-time approximation (cite).  (Provide algorithm here.)

For very large reference databases such as GenBank (which contains over 700,000
microbial genomes as of January 2021) and GTDB (XXX genomes in release XYZ), this is computationally prohibitive
to do exactly. (Estimate total number of k-mers in genbank!) We therefore implemented the algorithm using _Scaled MinHash_
sketches to estimate containment.

The results on two metagenomes, podar (used above) and p88mo11, an IBD
data set, are shown in figure XXX. (Should we add environmental,
e.g. hu-s1?)

![
**Hash-based decomposition of a metagenome into constituent genomes**
Each figure shows a rank ordering of the genomes chosen as likely
reference genomes, based on the maximum containment approach used by
our min-set-cov implementation. The Y is labeled with the name of the
genome (per NCBI), and the red circles indicates the number of
remaining k-mers (estimated with _Scaled MinHash_) shared between each
genome and the metagenome. The green x indicate the total number of k-mers
shared between each genome and the metagenome, including those already
assigned at previous ranks. (A) Gathergram for mock metagenome from Shakya
et al. (B) Gathergram for iHMP data set p880mo11. (C) Gathergram for 
oil well data set hu-s1.
](images/gathergram-SRR606249.hashes.svg "gather hash results for podar"){#fig:gather0}

Figure @fig:gather0 shows the results of this algorithm applied to three
metagenomes - one mock, one iHMP, and one environmental. The monotonically
decreasing assignments are as expected from the algorithm, which is greedy.



ZZ total numbers of genomes are identified.

XX and YY percent of metagenomes are identified.

Overlapping portions of genomes are identified. (Statistics of # k-mers, etc?)

This min-set-cov approach for assigning genomes to metagenomes using
k-mers differs substantially from extant k-mer and mapping-based
approaches.  LCA-based approaches such as Kraken assign taxonomy to
k-mers based on taxonomic lineages in a database, and then use the
resulting database of annotated k-mers to assign taxonomy to
individual reads or the metagenome in bulk. Mapping- and
homology-based approaches such as Diamond or @@@ use read mapping to
genomes or read alignment to gene sequences to assign taxonomy and
function. In contrast to the greedy min-set-cov approach described
here, which looks at the entire collection of reads/k-mers and assigns
them in aggregate to the best genome match, these approaches typically
focus on individual k-mers or reads.  It is not clear what the
implications of this is for taxonomy or function; we evaluate the
effects on taxonomy assignment below.

Another big advantage of the min-set-cov approach is its focus on
building a parsimonious list from complete genome databases. While
most extant approaches create a short, curated list of genomes, with
Scaled MinHash and sourmash awesomeness, it is now straightforward to
routinely search millions of genomes and boil them down to mere dozens
of genomes of relevance to a particular metagenome, following which
more compute-intensive approaches can be used for detailed
analysis. Of course large genome databases may suffer from problems of
contamination etc etc but at least this approach gives us the option.

TODO:

* Provide summaries of % k-mers identified/matched, etc.
* CTB: do we want to do this with all k-mers, not just scaled minhash? Or not. ralstonia or something? (the one taylor suggested.)
  
For discussion section:

* LCA is tied to taxonomy, not directly to genomes
* LCA saturates as database sizes grow - more k-mers get pulled up
* in contrast here, we identify combinatorial collections of k-mers in a greedy fashion. this basically means that we pull high-rank/multi-genome k-mers into the largest collection of genome-specific k-mers; we need to evaluate the consequences of this (and do so in the taxonomy seciotn, below).
* also note: LCA chooses discriminatory k-mers in advance, and when databases are updated they must also be updated; here we don't need to do that. We also don't need to a step of assigning taxonomy to k-mers.
* it is not clear how important this is comptuationally in terms of efficiency, given the tradeoffs of the min set cov algorithm, but it should be mentioned.

## K-mer decomposition of metagenomes approximates read mappability

K-mers have been widely used to approximate mapping (citations).  To
evaluate the utility of our min-set-cov approach, we ran min-set-cov
on three metagenomes and then mapped the metagenome reads to the
identified genomes.  To do this, we implemented a minimap-based version of
gather, in which we map all metagenome reads to all the genomes
identified by gather, and then iteratively subtract the reads that
mapped to the gather results in the order specified by gather and
remap them.

(CTB note: could also calculate this with mapping, but not against ALL
genomes, only against those already found with gather.)

Figure @fig:gather shows that mapping results generally correspond to gather
results.  However, they match more closely for synthetic communities
than for real communities, especially as gather rank increases.  This
is likely because in synthetic communities the reference genomes are
closer to the actual content of the metagenome, while in real
metagenomes we are mapping to imperfect references.

In particular, both the remnant k-mer and the remnant mappings
decrease substantially with increased gather rank.  This is because at
the higher ranks we are not mapping to all elements in the genome;
e.g. in figure XXX, we see that there is a substantial difference in
the total number of bases mapped vs the leftover reads from iterative
removal. Here only reads that did not map to higher ranked genomes are
mapping.

Inspection of the genome taxonomy show that in these situations, we
are mapping to subsets of genomes that are the same species or genus
as earlier ranked genomes. Figure XYZ compares the best-ranked
hash count to the aggregate hash count for the species pangenome; for
many species, the aggregate hashes identified for each
species in total far outweighs the hashes identified for any one
genome.

(belongs in discussion)
This suggests that metagenome reads are being mapped to different
genomic elements from a species pangenome. While we do not have the
resolution to determine this, the most parsimonious interpretation
is that the "true" reference genome for the species present in the
sample is not in the database, and instead is being cobbled together
from core and accessory genome elements in the database.

(Maybe this is where we use R. gnavus genomes? Yes - take JUST reads
that map to R. gnavus, do gather, show what happens x all gnavus
genomes? Could also do withholding, to show that pangenome elements will
usually map one way or another.)

(Show plots with leftover mapping vs all mapping.)

(maybe use sgc here? if so, this would be the last section!)

(CTB: revisit CMash/mash screen papers here to see how
they evaluated. Also, maybe mention sgc gbio paper and recovery of new
genome.)

![
**Hash-based decomposition of a metagenome into constituent
genomes compares well to bases covered by read mapping.** 
The reference genomes are rank ordered along the x axis based on the largest number of hashes from the metagenome specific to that genome, i.e. by order in gather output; hence the number of hashes classified for each genome (orange dots) is monotonically decreasing.
The y axis shows absolute number of estimated k-mers classified to this genome (orange) or total number of bases covered in the reference (blue); the numbers have not been rescaled.
Decreases in mapping (green peaks) occur for genomes which are not
exact matches to the genomes of the organisms used to build the mock
community (cite sherine, mash screen).
](images/gather-podar.svg "gather results for podar"){#fig:gather}

## Taxonomic profiling based on 'gather' is accurate

* CAMI results
* suggests gather/greedy decomposition is pretty good

We implement a lightweight taxonomic profiling method on top of gather
by directly transferring the taxonomies for the discovered genomes into
the profile. Lineages can then be summarized at each taxonomic rank.

To evaluate the performance of taxonomic profiling, we used the mouse
gut metagenome dataset [@meyer_tutorial_2020] from the Critical
Assessment of Metagenome Intepretation (CAMI)
[@sczyrba_critical_2017], a community-driven initiative for
reproducibly benchmarking metagenomic methods.
The simulated
mouse gut metagenome (_MGM_) was derived from 791 bacterial and
archaeal genomes,
representing 8 phyla,
18 classes,
26 orders,
50 families,
157 genera,
and 549 species.
64 samples were generated with _CAMISIM_,
with 91.8 genomes present on each sample on average.
Each sample is 5 GB in size, and both short-read (Illumina) and
long-read (PacBio) sequencing data is available.

![
Comparison per taxonomic rank of methods in terms of completeness, purity (1% filtered), and L1 norm.
](images/spider_plot_relative.svg){#fig:spider}

![
Performance per method at all major taxonomic ranks, with the shaded bands showing the standard deviation of a metric.  In **a** and **b**, completeness, purity, and L1 norm error range between 0 and 1.  The L1 norm error is normalized to this range and is also known as Bray-Curtis distance.  The higher the completeness and purity, and the lower the L1 norm, the better the profiling performance.
](images/ranks_by_tool.svg){#fig:ranks}

![
Methods rankings and scores obtained for the different metrics over all samples and taxonomic ranks.  For score calculation, all metrics were weighted equally.
](images/scores.svg){#fig:scores}


Figure @fig:spider, @fig:ranks, @fig:scores is an updated version of Figure 6 from [@meyer_tutorial_2020] including `sourmash`,
comparing 10 different methods for taxonomic profiling and their characteristics at each taxonomic rank. 
While previous methods show reduced completeness,
the ratio of taxa correctly identified in the ground truth,
below the genus level,
`sourmash` can reach 88.7\% completeness at the species level with the highest
purity (the ratio of correctly predicted taxa over all predicted taxa) across
all methods:
95.9\% when filtering predictions below 1\% abundance,
and 97\% for unfiltered results.
`sourmash` also has the lowest L1-norm error
(the sum of the absolute difference between the true and predicted abundances at
a specific taxonomic rank),
the highest number of true positives and the lowest number of false positives.

| Taxonomic binner                | Time (hh:mm) | Memory (kbytes) |
|:--------------------------------|-------------:|----------------:|
| MetaPhlAn 2.9.21                | 18:44        | 5,139,172       |
| MetaPhlAn 2.2.0                 | 12:30        | 1,741,304       |
| Bracken 2.5 (only Bracken)      | **0:01**     | **24,472**      |
| Bracken 2.5 (Kraken and Bracken)| **3:03**     | 39,439,796      |
| FOCUS 0.31                      | 13:27        | 5,236,199       |
| CAMIARKQuikr 1.0.0              | 16:19        | 27,391,555      |
| mOTUs 1.1                       | 19:50        | **1,251,296**   |
| mOTUs 2.5.1                     | 14:29        | 3,922,448       |
| MetaPalette 1.0.0               | 76:49        | 27,297,132      |
| TIPP 2.0.0                      | 151:01       | 70,789,939      |
| MetaPhyler 1.25                 | 119:30       |  2,684,720      |
| sourmash 3.4.0                  | 16:41        |  5,760,922      |

Table: Updated Supplementary Table 12 from [@meyer_tutorial_2020].
Elapsed (wall clock) time (h:mm) and maximum resident set size
(kbytes) of taxonomic profiling methods on the 64 short read samples
of the CAMI II mouse gut data set. The best results are shown in
bold. Bracken requires to run Kraken, hence the times required to run
Bracken and both tools are shown. The taxonomic profilers were run on
a computer with an Intel Xeon E5-4650 v4 CPU (virtualized to 16 CPU
cores, 1 thread per core) and 512 GB (536.870.912 kbytes) of main
memory. {#tbl:gather-cami2}

When considering resource consumption and running times, `sourmash`
used 5.62 GB of memory with an _LCA index_ built from the RefSeq
snapshot (141,677 genomes) with $scaled=10000$ and $k=51$.  Each
sample took 597 seconds to run (on average), totalling 10 hours and 37
minutes for 64 samples.  MetaPhlan 2.9.21 was also executed in the
same machine, a workstation with an AMD Ryzen 9 3900X 12-Core CPU
running at 3.80 GHz, 64 GB DDR4 2133 MHz of RAM and loading data from
an NVMe SSD, in order to compare to previously reported times in Table
@tbl:gather-cami2 [@meyer_tutorial_2020].  MetaPhlan took 11 hours and
25 minutes to run for all samples, compared to 18 hours and 44 minutes
previously reported, and correcting the `sourmash` running time by
this factor it would likely take 16 hours and 41 minutes in the
machine used in the original comparison.  After correction, `sourmash`
has similar runtime and memory consumption to the other best
performing tools (_mOTUs_ and _MetaPhlAn_), both gene marker and
alignment based tools.

Additional points are that `sourmash` is a single-threaded program, so
it didn't benefit from the 16 available CPU cores, and it is the only
tool that could use the full RefSeq snapshot, while the other tools
can only scale to a smaller fraction of it (or need custom databases).
The CAMI II RefSeq snapshot for reference genomes also doesn't include
viruses; this benefits `sourmash` because viral _Scaled MinHash_
sketches are usually not well supported for containment estimation,
since viral sequences require small scaled values to have enough
hashes to be reliable.

Notes:

* private database, private taxonomies are easily supported without reindexing.
