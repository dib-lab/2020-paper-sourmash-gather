# Results

## Scaled MinHash sketches support accurate containment operations

We define the Scaled MinHash on an input domain of $k$-mers, $W$, as follows:

$$\mathbf{SCALED}_s(W) = \{\,w \leq \frac{H}{s} \mid \forall w \in
W\,\}$$ where $H$ is the largest possible value in the domain of
$h(x)$ and $\frac{H}{s}$ is the \emph{maximum hash} value in the
Scaled MinHash.

The Scaled MinHash is a mix of MinHash and ModHash
[@doi:10.1109/SEQUEN.1997.666900].  It keeps the selection of the
smallest elements from MinHash, while using the dynamic size from
ModHash to allow containment estimation.  However, instead of taking
$0 \mod m$ elements like $\mathbf{MOD}_m(W)$, a Scaled MinHash uses the
parameter $s$ to select a subset of $W$.

Scaled MinHash supports containment estimation with high accuracy and
low bias. (Analytic work from David HERE.)

* approximation formula (eqn 13 from overleaf)
* for queries into large sets (large $|A|$), bias factor is low.
* refer to appendix for derivation.

Given a uniform hash function $h$ and $s=m$, the cardinalities of
$\mathbf{SCALED}_s(W)$ and $\mathbf{MOD}_m(W)$ converge for large
$\vert W \vert$.  The main difference is the range of possible values
in the hash space, since the Scaled MinHash range is contiguous and
the ModHash range is not.  This permits a variety of convenient
operations on the sketches, including iterative downsampling of Scaled
MinHash sketches as well as conversion to MinHash sketches.

## A Scaled MinHash implementation accurately estimates containment between sets of different sizes

We compare the _Scaled MinHash_ method to CMash (_Containment
MinHash_) [@doi:10.1101/184150] and Mash Screen (_Containment Score_)
[@doi:10.1186/s13059-019-1841-x] for containment queries in the
dataset from Shakya et al., 2014, a synthetic mock metagenomic
bacterial and archaeal community where the reference genomes are
largely known [@doi:10.1111/1462-2920.12086].  This data set has been
used in several methods evaluations
[@doi:10.1093/bioinformatics/btu395,@doi:10.1101/gr.213959.116,@doi:10.1101/155358,@awad_evaluating_2017,@doi:10.1186/s13059-019-1841-x].

![
**Letter-value plot [@hofmann_letter-value_2017] of the
differences from containment estimate to ground truth (exact).**
Each method is evaluated for $k=\{21,31,51\}$,
except for `Mash` with $k=51$, which is unsupported.
](images/containment.svg "Containment estimation between smol, CMash, and mash screen"){#fig:containment}

Figure @fig:containment shows results with low-coverage and
contaminant genomes (as described in [@awad_evaluating_2017] and
[@ondov_mash_2019]) removed from the database.
All methods are within 1\% of the exact containment on average (Figure
@fig:containment), with `CMash` consistently underestimating
the containment for large $k$ and overestimating for small $k$.  `Mash
Screen` with $n=10000$ has the smallest difference to ground truth for
$k=\{21, 31\}$, followed by `smol` with `scaled=1000` and `Mash
Screen` with $n=1000$.

CTB todo:

* use sourmash, not smol

CTB questions:

* should we add sketch sizes in here more explicitly? e.g. number of hashes kept?
* compares well with others
* How much is missed figure; Poisson calculations? => appendix?
* should we make comment here about sequencing errors?

## We can use Scaled Min-Hash to construct a minimum set cover for metagenomes

We next ask: what is the smallest collection of genomes in a database
that contains all of the known k-mers in a metagenome?
Formally, for a
given metagenome $M$ and a reference database $D$, what is the minimum
collection of genomes in $D$ which contain all of the k-mers in the
intersection of $D$ and $M$? That is, we wish to find the smallest set
$\{ G_n \}$ of genomes in $D$ such that $$k(M) \cap k(D) = \bigcup_n
\{ k(M) \cap k(G_n) \} $$

This is the *minimum set covering* problem, for which
there is a polynomial-time approximation (cite).  (Provide algorithm here.)

This greedy algorithm works by iteratively subtracting k-mers
belonging to the genome that has the highest containment count from
the metagenome (ref alg above).  This results in a progressive
classification of the known k-mers in the metagenome to specific
genomes, in rank order of number of contained hashes. Note that in
cases where equivalent matches are available at a particular rank,
the match is chosen at random.

In Figure @fig:gather0, we show the results of this iterative
decomposition of the mock metagenome from Shakya et al., 2014, into
constituent genome matches.  The high rank (early) matches reflect
large and/or mostly-covered genomes with high containment, while later
matches reflect smaller genomes, lower-covered genomes, and/or genomes
with substantial overlap with earlier matches. Where there are
overlaps between genomes, shared common k-mers are "claimed" by higher
rank matches and only k-mer content specific to the later genome is
used to identify the lower rank matches. For example, two strains of
*Shewanella baltica* present in the mock metagenome in Figure
@fig:gather0 have appproximately 50% overlap in k-mer content, and
these shared k-mers are claimed by *Shewanella baltica* OS223 (compare
*Shewanella baltica* OS223, rank 8, with *Shewanella baltica* OS185,
rank 33; the difference between the red circles and green triangles
for *S. baltica* OS185 represents the k-mers claimed by *S. baltica*
OS223).

For this mock metagenome, 205m (54.8%) of 375m k-mers were found in
GenBank.  The remaining 169m (45.2%) k-mers had no matches, and
represent either error k-mers from sequencing or unknown k-mers from
real community members.

![
**K-mer decomposition of a metagenome into constituent genomes.**
A rank ordering for the first 36 genomes from the minimum set cover
of the synthetic metagenome from Shakya et al., calculated using 700,000
GenBank genomes. The Y axis is labeled with the NCBI-designed name of the
genome.
In the left plot, the X axis represents the estimated number of k-mers shared
between each genome and the metagenome. The red circles indicate the number
of remaining k-mers at that rank, while the green triangle symbols indicate
the total number of k-mers, including those already assigned at previous ranks.
In the right plot, the X axis represents the estimated k-mer coverage of that
genome.  The red circles indicate the coverage with the remaining k-mers at
that rank, while the green triangle symbols indicate total coverage with all
k-mers in the metagenome, including those already assigned at previous ranks.
](images/gathergram-SRR606249.hashes.svg "minimum set cover for podar"){#fig:gather0}

## Minimum metagenome covers can accurately estimate taxonomic composition

We evaluated the accuracy of min-set-cov for metagenome decomposition
by using benchmarks from the Critical Asssessment of Metagenome
Interpretation (CAMI) [@doi:10.1038/nmeth.4458], a community-driven
initiative for reproducibly benchmarking metagenomic methods.  We used
the mouse gut metagenome dataset [@doi:10.1038/s41596-020-00480-3],
in which a simulated
mouse gut metagenome (_MGM_) was derived from 791 bacterial and
archaeal genomes,
representing 8 phyla,
18 classes,
26 orders,
50 families,
157 genera,
and 549 species.
64 samples were generated with _CAMISIM_,
with 91.8 genomes present on each sample on average.
Each sample is 5 GB in size, and both short-read (Illumina) and
long-read (PacBio) simulated sequencing data is available.
(CTB: check citations / content of latest actual CAMI pub.)

Since min-set-cov yields only a collection of genomes rather than a
species list, we generated a taxonomic profile for a given metagenome
cover. For each genome match, we used the species designation in the
NCBI taxonomy for that genome. Then, we calculated the fraction of the
genome remaining in the metagenome after k-mers belonging to
higher-rank genomes have been removed (red circles in Figure @fig:gather0
(a)). We used this fraction to weight the contribution of the genome's
species designation towards the metagenome taxonomy. This procedure
produces an estimate of that species' taxonomic contribution to the
metagenome, normalized by the genome size.

![
Comparison per taxonomic rank of methods in terms of completeness, purity (1% filtered), and L1 norm.
](images/spider_plot_relative.svg){#fig:spider}

<!--
![
Performance per method at all major taxonomic ranks, with the shaded bands showing the standard deviation of a metric.  In **a** and **b**, completeness, purity, and L1 norm error range between 0 and 1.  The L1 norm error is normalized to this range and is also known as Bray-Curtis distance.  The higher the completeness and purity, and the lower the L1 norm, the better the profiling performance.
](images/ranks_by_tool.svg){#fig:ranks}
-->

![
Methods rankings and scores obtained for the different metrics over all samples and taxonomic ranks.  For score calculation, all metrics were weighted equally.
](images/scores.svg){#fig:scores}


In Figures @fig:spider and @fig:scores we show an updated version of
Figure 6 from [@doi:10.1038/s41596-020-00480-3] that includes our
method, implemented in the `sourmash` software (CTB: what databases
are used?). Here we compare 10 different methods for taxonomic
profiling and their characteristics at each taxonomic rank.  While
previous methods show reduced completeness, the ratio of taxa
correctly identified in the ground truth, below the genus level,
`sourmash` can reach 88.7\% completeness at the species level with the
highest purity (the ratio of correctly predicted taxa over all
predicted taxa) across all methods: 95.9\% when filtering predictions
below 1\% abundance, and 97\% for unfiltered results.  `sourmash` also
has the lowest L1-norm error (the sum of the absolute difference
between the true and predicted abundances at a specific taxonomic
rank), the highest number of true positives and the lowest number of
false positives.

<!--
| Taxonomic binner                | Time (hh:mm) | Memory (kbytes) |
|:--------------------------------|-------------:|----------------:|
| MetaPhlAn 2.9.21                | 18:44        | 5,139,172       |
| MetaPhlAn 2.2.0                 | 12:30        | 1,741,304       |
| Bracken 2.5 (only Bracken)      | **0:01**     | **24,472**      |
| Bracken 2.5 (Kraken and Bracken)| **3:03**     | 39,439,796      |
| FOCUS 0.31                      | 13:27        | 5,236,199       |
| CAMIARKQuikr 1.0.0              | 16:19        | 27,391,555      |
| mOTUs 1.1                       | 19:50        | **1,251,296**   |
| mOTUs 2.5.1                     | 14:29        | 3,922,448       |
| MetaPalette 1.0.0               | 76:49        | 27,297,132      |
| TIPP 2.0.0                      | 151:01       | 70,789,939      |
| MetaPhyler 1.25                 | 119:30       |  2,684,720      |
| sourmash 3.4.0                  | 16:41        |  5,760,922      |

Table: Updated Supplementary Table 12 from [@meyer_tutorial_2020].
Elapsed (wall clock) time (h:mm) and maximum resident set size
(kbytes) of taxonomic profiling methods on the 64 short read samples
of the CAMI II mouse gut data set. The best results are shown in
bold. Bracken requires to run Kraken, hence the times required to run
Bracken and both tools are shown. The taxonomic profilers were run on
a computer with an Intel Xeon E5-4650 v4 CPU (virtualized to 16 CPU
cores, 1 thread per core) and 512 GB (536.870.912 kbytes) of main
memory. {#tbl:gather-cami2}

When considering resource consumption and running times, `sourmash`
used 5.62 GB of memory with an _LCA index_ built from the RefSeq
snapshot (141,677 genomes) with $scaled=10000$ and $k=51$.  Each
sample took 597 seconds to run (on average), totalling 10 hours and 37
minutes for 64 samples.  MetaPhlan 2.9.21 was also executed in the
same machine, a workstation with an AMD Ryzen 9 3900X 12-Core CPU
running at 3.80 GHz, 64 GB DDR4 2133 MHz of RAM and loading data from
an NVMe SSD, in order to compare to previously reported times in Table
@tbl:gather-cami2 [@meyer_tutorial_2020].  MetaPhlan took 11 hours and
25 minutes to run for all samples, compared to 18 hours and 44 minutes
previously reported, and correcting the `sourmash` running time by
this factor it would likely take 16 hours and 41 minutes in the
machine used in the original comparison.  After correction, `sourmash`
has similar runtime and memory consumption to the other best
performing tools (_mOTUs_ and _MetaPhlAn_), both gene marker and
alignment based tools.

Additional points are that `sourmash` is a single-threaded program, so
it didn't benefit from the 16 available CPU cores, and it is the only
tool that could use the full RefSeq snapshot, while the other tools
can only scale to a smaller fraction of it (or need custom databases).
The CAMI II RefSeq snapshot for reference genomes also doesn't include
viruses; this benefits `sourmash` because viral _Scaled MinHash_
sketches are usually not well supported for containment estimation,
since viral sequences require small scaled values to have enough
hashes to be reliable.

-->

Notes:

* private database, private taxonomies are easily supported without reindexing.

## Minimum metagenome covers select small subsets of large databases

Something something modern databases often have large redundanc.

<!--
For very large reference databases such as GenBank (which contains
over 700,000 microbial genomes as of January 2021) and GTDB (230,000
genomes in release RS202), this is computationally challenging to do
exactly. (Estimate total number of k-mers in genbank!) We therefore
implemented the algorithm using _Scaled MinHash_ sketches to estimate
containment, and used an overlap threshold of 100,000 k-mers in order
to eliminate genomes with only small overlaps (see Methods).
-->

| data set | genomes >= 100k overlap | min-set-cov | % k-mers identified |
| -------- | -------- | -------- | ------- | 
| zymo mock (SRR12324253) | 405,839 | 19 | 0% |
| podar mock (SRR606249) | 5800 | 74 | 0% |
| p8808mo11 (iHMP)  | 96,423     | 99     | 0% |
| hu-s1 oil well (SRR1976948) | 1235 | 135 | 0% |

Table: metagenomes and min-set-cov. {#tbl:genbank-cover}

In Table @tbl:genbank-cover, we show the results of running min-set-cov
for four metagenomes against genbank - two mock communities (cite
cite), one human microbiome data set from iHMP (cite), and one oil
well (cite).  Our implementation provides estimates for both the
*total* number of genomes with substantial overlap to a query genome,
and a *minimum list* of genomes that account for k-mers with overlap
in the query metagenome (see Methods - prefetch and gather).

We find many genomes with large overlaps, due to the
redundancy of the database. For example, the zymo mock contains a
*Salmonella* genome, and there are over 200,000 Salmonella genomes
that match to it in Genbank. Likewise, the iHMP dataset contains many
XYZ.  Since neither the podar mock nor the oil well community contain
genomes from species with substantial strain representation in
genbank, they have many fewer total overlaps.

However, regardless of the number of genome with overlap, the
estimated _minimum_ collection of genomes is always much smaller. In
the cases where the k-mers in the metagenome are mostly identified,
this is because of database redundancy: e.g. in the case of the zymo
mock, the min-set-cov algorithm chooses only one Salmonella genome
from the 200,000+ available. Conversely, in the case of the oil well
sample, much of the sample is not identified,
suggesting that the small size of the covering set is because much
of the sample is not represented in the database.

CTB TODO: add % identified to table!

## Minimum metagenome covers provide representative genomes for mapping

Mapping metagenome reads to representative genomes is an important
step in many microbiome analysis pipelines, but mapping approaches
struggle with large, redundant databases.  One use case for a minimum
metagenome cover is to select a small set of representative genomes to
be used for mapping.  We therefore developed a hybrid selection and
mapping pipeline that uses the rank-ordered min-set-cov results to
map reads to candidate genomes.

We first map all reads to all genomes in the minimum set cover, and
then successively remove reads that map to higher rank genomes from
lower rank genomes, and remap the remaining reads. That is, all reads
mapped to the rank-1 genome in Figure @fig:gather0 are removed from
the rank-2 genome mapping, and all reads mapping to rank-1 and rank-2
genomes are removed from the rank-3 genome mapping. This produces
results directly analogous to those presented in Figure @fig:gather0,
but for reads rather than k-mers (CTB: provide as Suppl Figure?).
Importantly, in this process we only consider genomes identified in
the minimum set cover, because it is computationally intractable to map
reads to the entire GenBank database.

CTB: note somewhere that we choose arbitrarily between equivalent matches
in the min-set-cov algorithm.

Figure @fig:mapping compares hash assignment rates
and mapping rates for the four evaluation metagenomes in Table
@tbl:genbank-cover. Broadly speaking, we see that k-mer/hash-based
estimates of metagenome composition align closely with the number of
bases covered by mapped reads. This suggests that the k-mer based min-set-cov
approach effectively selects reference genomes for metagenome read mapping.

For mock metagenomes (panels X and Y), there appears to be a close
correspondence between mapping and hash assignment rates, while for
actual metagenomes, there is more variation between mapping and hash
assignments.  Further work is needed to evaluate rates of variation across
a larger number of metagenomes.

CTB: do this for all four metagenomes!

<!--
(belongs in discussion)
This suggests that metagenome reads are being mapped to different
genomic elements from a species pangenome. While we do not have the
resolution to determine this, the most parsimonious interpretation
is that the "true" reference genome for the species present in the
sample is not in the database, and instead is being cobbled together
from core and accessory genome elements in the database.

(Maybe this is where we use R. gnavus genomes? Yes - take JUST reads
that map to R. gnavus, do gather, show what happens x all gnavus
genomes? Could also do withholding, to show that pangenome elements will
usually map one way or another.)

(Show plots with leftover mapping vs all mapping.)

-->

![
**Hash-based decomposition of a metagenome into constituent
genomes compares well to bases covered by read mapping.** 
The reference genomes are rank ordered along the x axis based on the largest number of hashes from the metagenome specific to that genome, i.e. by order in gather output; hence the number of hashes classified for each genome (orange dots) is monotonically decreasing.
The y axis shows absolute number of estimated k-mers classified to this genome (orange) or total number of bases covered in the reference (blue); the numbers have not been rescaled.
Decreases in mapping (green peaks) occur for genomes which are not
exact matches to the genomes of the organisms used to build the mock
community (cite sherine, mash screen).
](images/gather-podar.svg "gather results for podar"){#fig:mapping}

