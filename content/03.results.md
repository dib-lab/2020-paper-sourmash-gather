# Results

We first describe _FracMinHash_, a sketching technique that supports
containment estimation for metagenome datasets using k-mers. We next
frame reference-based metagenome content analysis as the problem of
finding a _minimum set cover_ for a metagenome using a collection of
reference genomes. We then evaluate the accuracy of this approach
using a taxonomic classification benchmark. Finally, we demonstrate
the utility of this approach by using the genomes from the minimum metagenome
cover as reference genomes for read mapping.

## FracMinHash sketches support accurate containment operations

We define the *fractional MinHash*, or FracMinHash, on an input domain
of hash values $W$, as follows:

$$\mathbf{FRAC}_s(W) = \{\,w \leq \frac{H}{s} \mid \forall w \in
W\,\}$$ where $H$ is the largest possible value in the domain of
$h(x)$ and $\frac{H}{s}$ is the *maximum hash value* allowed in the
FracMinHash sketch.

The FracMinHash is a mix of MinHash and ModHash
[@doi:10.1109/SEQUEN.1997.666900].  It keeps the selection of the
smallest elements from MinHash, while using the dynamic size from
ModHash to allow containment estimation.  However, instead of taking
$0 \mod m$ elements like $\mathbf{MOD}_m(W)$, a FracMinHash uses the
parameter $s$ to select a subset of $W$.

<!--

FracMinHash supports containment estimation with high accuracy and
low bias. **(Analytic work from David HERE.)**

* approximation formula (eqn 13 from overleaf)
* for queries into large sets (large $|A|$), bias factor is low.
* refer to appendix for derivation.

-->

Given a uniform hash function $h$ and $s=m$, the cardinalities of
$\mathbf{FRAC}_s(W)$ and $\mathbf{MOD}_m(W)$ converge for large
$\vert W \vert$.  The main difference is the range of possible values
in the hash space, since the FracMinHash range is contiguous and
the ModHash range is not.  This permits a variety of convenient
operations on the sketches, including iterative downsampling of FracMinHash sketches as well as conversion to MinHash sketches.

## A FracMinHash implementation accurately estimates containment between sets of different sizes

We compare the FracMinHash method to CMash (_Containment
MinHash_) [@doi:10.1101/184150] and Mash Screen (_Containment Score_)
[@doi:10.1186/s13059-019-1841-x] for containment queries in data from a
mock bacterial and archaeal community where the
reference genomes are largely known [@doi:10.1111/1462-2920.12086].
This data set has been used in several methods evaluations
[@doi:10.1093/bioinformatics/btu395;@doi:10.1101/gr.213959.116;@doi:10.1101/155358;@doi:10.1186/s13059-019-1841-x].

![
**Letter-value plot [@doi:10.1080/10618600.2017.1305277] of the
differences from containment estimate to ground truth (exact).**
Each method is evaluated for $k=\{21,31,51\}$,
except for `Mash` with $k=51$, which is unsupported.
](images/containment.svg "Containment estimation between FracMinHash, CMash, and mash screen"){#fig:containment}

Figure @fig:containment shows containment analysis of genomes in this metagenome, with low-coverage and
contaminant genomes (as described in [@doi:10.1101/155358] and
[@doi:10.1186/s13059-016-0997-x]) removed from the database.
All methods are within 1\% of the exact containment on average (Figure
@fig:containment), with `CMash` consistently underestimating
the containment for large $k$ and overestimating for small $k$.  `Mash
Screen` with $n=10000$ has the smallest difference to ground truth for
$k=\{21, 31\}$, followed by `FracMinHash` with `scaled=1000` and `Mash
Screen` with $n=1000$.

CTB TODO:

* switch figure to use sourmash; update description to reference sourmash.

## We can use FracMinHash to construct a minimum set cover for metagenomes

We next ask: what is the smallest collection of genomes in a database
that contains all of the known k-mers in a metagenome?
Formally, for a
given metagenome $M$ and a reference database $D$, what is the minimum
collection of genomes in $D$ which contain all of the k-mers in the
intersection of $D$ and $M$? We wish to find the smallest set
$\{ G_n \}$ of genomes in $D$ such that, for the k-mer decomposition $k()$,
$$ k(M) \cap k(D) = \bigcup_n \{ k(M) \cap k(G_n) \} $$

This is a *minimum set covering* problem, for which there is a
polynomial-time approximation [@doi:10.1007/978-0-387-30162-4_175]:

1. Initialize $C \leftarrow \emptyset$
2. Define $f(C) = \vert \cup_{s \in C} \{ s \} \vert$
3. Repeat until $f(C) = f(M \cap D)$:
   4. Choose $s \in G$ maximizing the contribution of the element $f(C \cup \{ s \}) - f(C)$
   5. Let $C \leftarrow C \cup \{ s \}$
6. Return $C$

This greedy algorithm iteratively chooses reference genomes from $D$
in order of largest remaining overlap with $M$.  This results in a
progressive classification of the known k-mers in the
metagenome to specific genomes.[^equivalent]

[^equivalent]: In our current implementation in `sourmash`, when
equivalent matches are available for a given rank, a match is chosen
at random. This is an implementation decision that is not intrinsic to
the algorithm itself.

In Figure @fig:gather0, we show an example of this iterative
classification of k-mers by matching GenBank genome for the mock metagenome from 
[@doi:10.1111/1462-2920.12086], which we term `podar mock` (see
Table @tbl:genbank-cover, row 2). The matching genomes are provided
in the order found by the greedy algorithm, i.e. by overlap with remaining k-mers in the metagenome.
The
high rank (early) matches reflect large and/or mostly-covered genomes
with high containment, while later matches reflect genomes that share
fewer k-mers with the remaining set of k-mers in the metagenome -
smaller genomes, less-covered genomes, and/or genomes with substantial
overlap with earlier matches. Where there are overlaps between
genomes, shared common k-mers are "claimed" by higher rank matches and
only k-mer content specific to the later genome is used to find
lower rank matches.

As one example of metagenome k-mers shared with multiple matches,
genomes from two strains of *Shewanella baltica* are present in the
mock metagenome.  These genomes have an approximately 50% overlap in
k-mer content, and these shared k-mers are first claimed by
*Shewanella baltica* OS223 -- compare *S. baltica* OS223, rank
8, with *S. baltica* OS185, rank 33 in Figure
@fig:gather0. Here the difference between the red circles and green
triangles for *S. baltica* OS185 represents the k-mers claimed by
*S. baltica* OS223 .

<!-- (CTB: maybe indicate or highlight these genomes in the figure?) -->

For this mock metagenome, 205m (54.8%) of 375m k-mers were found in
GenBank
(see
Table @tbl:genbank-cover, row 2).  The remaining 169m (45.2%) k-mers had no matches, and
represent either k-mers introduced by sequencing errors or unknown k-mers from
real community members.

![
**K-mer decomposition of a metagenome into constituent genomes.**
A rank ordering by remaining containment for the first 36 genomes from the minimum metagenome cover
of the `podar mock` synthetic metagenome [@doi:10.1111/1462-2920.12086],
calculated using 700,000 genomes from GenBank. The Y axis is labeled with the NCBI-designed name of the
genome.
In the left plot, the X axis represents the estimated number of k-mers shared
between each genome and the metagenome. The red circles indicate the number
of matching k-mers that were not matched at previous ranks, while the green triangle symbols indicate all matching k-mers.
In the right plot, the X axis represents the estimated k-mer coverage of that
genome.  The red circles indicate the percentage of the genome covered by k-mers remaining at
that rank, while the green triangle symbols indicate overlap between
the genome and the entire metagenome, including those already assigned at previous ranks.
](images/gathergram-SRR606249.hashes.svg "minimum metagenome cover for podar"){#fig:gather0}

## Minimum metagenome covers can accurately estimate taxonomic composition

We evaluated the accuracy of min-set-cov for metagenome decomposition
using benchmarks from the Critical Assessment of Metagenome
Interpretation (CAMI) [@doi:10.1038/nmeth.4458], a community-driven
initiative for reproducibly benchmarking metagenomic methods.  We used
the mouse gut metagenome dataset [@doi:10.1038/s41596-020-00480-3],
in which a simulated
mouse gut metagenome (_MGM_) was derived from 791 bacterial and
archaeal genomes,
representing 8 phyla,
18 classes,
26 orders,
50 families,
157 genera,
and 549 species.
64 samples were generated with _CAMISIM_,
with 91.8 genomes present on each sample on average.
Each sample is 5 GB in size, and both short-read (Illumina) and
long-read (PacBio) simulated sequencing data is available.
(CTB: check citations / content of latest actual CAMI pub, https://www.biorxiv.org/content/10.1101/2021.07.12.451567v1)

Since min-set-cov yields only a collection of genomes, it must be
converted into a taxonomy for benchmarking with CAMI.
We developed the following procedure for
generating a taxonomic profile from a given metagenome
cover. For each genome match, we note
the species designation in the NCBI taxonomy for that genome. Then, we
calculate the fraction of the genome remaining in the metagenome
after k-mers belonging to higher-rank genomes have been removed (e.g. red
circles in Figure @fig:gather0 (a)). We use this fraction to weight
the contribution of the genome's species designation to the
metagenome taxonomy. This procedure produces an estimate of that
species' taxonomic contribution to the metagenome, and normalizes by the
genome size.

![
Comparison per taxonomic rank of methods in terms of completeness, purity (1% filtered), and L1 norm.
](images/spider_plot_relative.svg){#fig:spider}

<!--
![
Performance per method at all major taxonomic ranks, with the shaded bands showing the standard deviation of a metric.  In **a** and **b**, completeness, purity, and L1 norm error range between 0 and 1.  The L1 norm error is normalized to this range and is also known as Bray-Curtis distance.  The higher the completeness and purity, and the lower the L1 norm, the better the profiling performance.
](images/ranks_by_tool.svg){#fig:ranks}
-->

![
Methods rankings and scores obtained for the different metrics over all samples and taxonomic ranks.  For score calculation, all metrics were weighted equally.
](images/scores.svg){#fig:scores}


In Figures @fig:spider and @fig:scores we show an updated version of
Figure 6 from [@doi:10.1038/s41596-020-00480-3] that includes our
method, implemented in the `sourmash` software (CTB: what databases
are used?). Here we compare 10 different methods for taxonomic
profiling and their characteristics at each taxonomic rank.  While
previous methods show reduced completeness -- the ratio of taxa
correctly identified in the ground truth -- below the genus level,
`sourmash` can reach 88.7\% completeness at the species level with the
highest purity (the ratio of correctly predicted taxa over all
predicted taxa) across all methods: 95.9\% when filtering predictions
below 1\% abundance, and 97\% for unfiltered results.  `sourmash` also
has the second lowest L1-norm error,
<!-- (the sum of the absolute difference
between the true and predicted abundances at a specific taxonomic
rank) -->
the highest number of true positives and the lowest number of
false positives.
<!-- TR: it looks like mOTU has hte lowest L1-norm, not sourmash? -->
<!-- runtimes/memory are mentioned in the discussion saying they're in the appendix,
I think it would be nice to mention that they're in the appendix here -->

<!--
| Taxonomic binner                | Time (hh:mm) | Memory (kbytes) |
|:--------------------------------|-------------:|----------------:|
| MetaPhlAn 2.9.21                | 18:44        | 5,139,172       |
| MetaPhlAn 2.2.0                 | 12:30        | 1,741,304       |
| Bracken 2.5 (only Bracken)      | **0:01**     | **24,472**      |
| Bracken 2.5 (Kraken and Bracken)| **3:03**     | 39,439,796      |
| FOCUS 0.31                      | 13:27        | 5,236,199       |
| CAMIARKQuikr 1.0.0              | 16:19        | 27,391,555      |
| mOTUs 1.1                       | 19:50        | **1,251,296**   |
| mOTUs 2.5.1                     | 14:29        | 3,922,448       |
| MetaPalette 1.0.0               | 76:49        | 27,297,132      |
| TIPP 2.0.0                      | 151:01       | 70,789,939      |
| MetaPhyler 1.25                 | 119:30       |  2,684,720      |
| sourmash 3.4.0                  | 16:41        |  5,760,922      |

Table: Updated Supplementary Table 12 from [@meyer_tutorial_2020].
Elapsed (wall clock) time (h:mm) and maximum resident set size
(kbytes) of taxonomic profiling methods on the 64 short read samples
of the CAMI II mouse gut data set. The best results are shown in
bold. Bracken requires to run Kraken, hence the times required to run
Bracken and both tools are shown. The taxonomic profilers were run on
a computer with an Intel Xeon E5-4650 v4 CPU (virtualized to 16 CPU
cores, 1 thread per core) and 512 GB (536.870.912 kbytes) of main
memory. {#tbl:gather-cami2}

When considering resource consumption and running times, `sourmash`
used 5.62 GB of memory with an _LCA index_ built from the RefSeq
snapshot (141,677 genomes) with $scaled=10000$ and $k=51$.  Each
sample took 597 seconds to run (on average), totaling 10 hours and 37
minutes for 64 samples.  MetaPhlan 2.9.21 was also executed in the
same machine, a workstation with an AMD Ryzen 9 3900X 12-Core CPU
running at 3.80 GHz, 64 GB DDR4 2133 MHz of RAM and loading data from
an NVMe SSD, in order to compare to previously reported times in Table
@tbl:gather-cami2 [@meyer_tutorial_2020].  MetaPhlan took 11 hours and
25 minutes to run for all samples, compared to 18 hours and 44 minutes
previously reported, and correcting the `sourmash` running time by
this factor it would likely take 16 hours and 41 minutes in the
machine used in the original comparison.  After correction, `sourmash`
has similar runtime and memory consumption to the other best
performing tools (_mOTUs_ and _MetaPhlAn_), both gene marker and
alignment based tools.

Additional points are that `sourmash` is a single-threaded program, so
it didn't benefit from the 16 available CPU cores, and it is the only
tool that could use the full RefSeq snapshot, while the other tools
can only scale to a smaller fraction of it (or need custom databases).
The CAMI II RefSeq snapshot for reference genomes also doesn't include
viruses; this benefits `sourmash` because viral _Scaled MinHash_
sketches are usually not well supported for containment estimation,
since viral sequences require small scaled values to have enough
hashes to be reliable.

-->

## Minimum metagenome covers select small subsets of large databases

<!--
For very large reference databases such as GenBank (which contains
over 700,000 microbial genomes as of January 2021) and GTDB (230,000
genomes in release RS202), this is computationally challenging to do
exactly. (Estimate total number of k-mers in genbank!) We therefore
implemented the algorithm using _Scaled MinHash_ sketches to estimate
containment, and used an overlap threshold of 100,000 k-mers in order
to eliminate genomes with only small overlaps (see Methods).
-->

| data set | genomes >= 100k overlap | min-set-cov | % k-mers identified |
| -------- | -------- | -------- | ------- | 
| `zymo mock` (SRR12324253) | 405,839 | 19 | 47.1% |
| `podar mock` (SRR606249) | 5800 | 74 | 54.8% |
| `gut real` (SRR5650070)  | 96,423     | 99     | 36.0% |
| `oil well real` (SRR1976948) | 1235 | 135 | 14.9% |

Table: Four metagenomes and the number of genomes in the estimated minimum metagenome cover from GenBank. {#tbl:genbank-cover}

In Table @tbl:genbank-cover, we show the minimum metagenome cover
for four metagenomes against GenBank - two mock communities 
[@https://www.zymoresearch.com/collections/zymobiomics-microbial-community-standards;@doi:10.1111/1462-2920.12086], a human gut microbiome data set
from iHMP [@doi:10.1038/s41586-019-1238-8], and an oil well sample
[@doi:10.1128/mBio.01669-15].  Our implementation provides estimates
for both the *total* number of genomes with substantial overlap to a
query genome, and the minimum set of genomes that account for k-mers
with overlap in the query metagenome. Note that only matches estimated
to have more than 100,000 overlapping k-mers are shown (see Methods for
details).

We find many genomes with overlaps for each metagenome, due to
the redundancy of the reference database. For example, `zymo mock`
contains a *Salmonella* genome, and there are over 200,000 *Salmonella*
genomes that match to it in GenBank. Likewise, `gut real`
matches to over 75,000 *E. coli* genomes in GenBank.  Since neither
`podar mock` nor `oil well real` contain genomes from species with
substantial representation in GenBank, they yield many fewer total
overlapping genomes.

Regardless of the number of genomes in the database with substantial
overlap, the
estimated _minimum_ collection of genomes is always much smaller than the
number of genomes with overlaps. In
the cases where the k-mers in the metagenome are mostly identified,
this is because of database redundancy: e.g. in the case of `zymo
mock`, the min-set-cov algorithm chooses precisely one *Salmonella* genome
from the 200,000+ available. Conversely, in the case of `oil well real`,
much of the sample is not identified,
suggesting that the small size of the covering set is because much
of the sample is not represented in the database.

<!-- CTB provide some kind of taxonomic breakdown at, say, genus level? -->

## Minimum metagenome covers provide representative genomes for mapping

Mapping metagenome reads to representative genomes is an important
step in many microbiome analysis pipelines, but mapping approaches
struggle with large, redundant databases.  One specific use for a minimum
metagenome cover could be to select a small set of representative genomes
for mapping.  We therefore developed a hybrid selection and
mapping pipeline that uses the rank-ordered min-set-cov results to
map reads to candidate genomes.

<!-- CTB do we have a citation for "mapping approaches struggle..."? -->

We first map all metagenome reads to the first ranked genome in the minimum metagenome
cover, and then remove successfully mapped reads from the metagenome.
Remaining unmapped reads are then mapped to the second rank genome, and
this then continues until all genomes have been used.
That is, all reads
mapped to the rank-1 genome in Figure @fig:gather0 are removed from
the rank-2 genome mapping, and all reads mapping to rank-1 and rank-2
genomes are removed from the rank-3 genome mapping, and so on. This produces
results directly analogous to those presented in Figure @fig:gather0,
but for reads rather than k-mers.
<!-- Importantly, in this process we only consider genomes identified in
the minimum set cover, because it is computationally intractable to map
reads to the entire GenBank database. (CTB: check centrifuge.) 
(CTB: provide versions of figure 1 here as Suppl Figure?) 
-->

Figure @fig:mapping compares hash assignment rates
and mapping rates for the four evaluation metagenomes in Table
@tbl:genbank-cover. Broadly speaking, we see that k-mer-based
estimates of metagenome composition agree closely with the number of
bases covered by mapped reads: the y axis has not been re-scaled, so hash matches and read mapping coverage correspond well. This suggests that the k-mer-based min-set-cov
approach effectively selects reference genomes for metagenome read mapping.

For mock metagenomes (panels X and Y), there appears to be a close
correspondence between mapping and hash assignment rates, while for
actual metagenomes, there is more variation between mapping and hash
assignments.  Further work is needed to evaluate rates of variation across
a larger number of metagenomes.

CTB: update figure to contain all four metagenomes! Fix axis labels, symbols.

<!--
(belongs in discussion)
This suggests that metagenome reads are being mapped to different
genomic elements from a species pangenome. While we do not have the
resolution to determine this, the most parsimonious interpretation
is that the "true" reference genome for the species present in the
sample is not in the database, and instead is being cobbled together
from core and accessory genome elements in the database.

(Maybe this is where we use R. gnavus genomes? Yes - take JUST reads
that map to R. gnavus, do gather, show what happens x all gnavus
genomes? Could also do withholding, to show that pangenome elements will
usually map one way or another.)

(Show plots with leftover mapping vs all mapping.)

-->

![
**Hash-based k-mer decomposition of a metagenome into constituent
genomes compares well to bases covered by read mapping.** 
(CTB: add more description here.)
Plots for each of four metagenomes showing estimated k-mer overlap per
genome, along
with bases covered by read mapping.
The reference genomes are rank ordered along the x axis (as in Figure @fig:gather0), based on the largest number of hashes from the metagenome specific to that genome; hence the number of hashes classified for each genome (red circles) is monotonically decreasing.
The y axis shows number of hashes (k-mers) classified to this genome (red circles) or total number of bases in the reference covered by mapped reads (blue stars); the numbers have not been rescaled.
Decreases in mapping (green peaks) occur for genomes which are not
exact matches to the genomes of the organisms used to build the mock
community; for example, in plot (a), the peak at rank 33, *S. baltica OS185* represents reads
that were preferentially mapped to *S. baltica OS223*, rank 8.
[@doi:10.1101/155358;@doi:10.1186/s13059-016-0997-x].
](images/gather-podar.svg "gather results for podar"){#fig:mapping}
