# Results

## Scaled MinHash sketches support accurate containment operations

We define the Scaled MinHash on an input domain of $k$-mers, $W$, as follows:

$$\mathbf{SCALED}_s(W) = \{\,w \leq \frac{H}{s} \mid \forall w \in
W\,\}$$ where $H$ is the largest possible value in the domain of
$h(x)$ and $\frac{H}{s}$ is the \emph{maximum hash} value in the
Scaled MinHash.

The Scaled MinHash is a mix of MinHash and ModHash
[@doi:10.1109/SEQUEN.1997.666900].  It keeps the selection of the
smallest elements from MinHash, while using the dynamic size from
ModHash to allow containment estimation.  However, instead of taking
$0 \mod m$ elements like $\mathbf{MOD}_m(W)$, a Scaled MinHash uses the
parameter $s$ to select a subset of $W$.

Scaled MinHash supports containment estimation with high accuracy and
low bias. (Analytic work from David HERE.)

* approximation formula (eqn 13 from overleaf)
* for queries into large sets (large $|A|$), bias factor is low.
* refer to appendix for derivation.

Given a uniform hash function $h$ and $s=m$, the cardinalities of
$\mathbf{SCALED}_s(W)$ and $\mathbf{MOD}_m(W)$ converge for large
$\vert W \vert$.  The main difference is the range of possible values
in the hash space, since the Scaled MinHash range is contiguous and
the ModHash range is not.  This permits a variety of convenient
operations on the sketches, including iterative downsampling of Scaled
MinHash sketches as well as conversion to MinHash sketches.

## Scaled MinHash accurately estimates containment between sets of different sizes

We compare the _Scaled MinHash_ method to
CMash (_Containment MinHash_) and Mash Screen (_Containment Score_)
for containment queries in the Shakya dataset
[@doi:10.1111/1462-2920.12086 ], a synthetic mock metagenomic
bacterial and archaeal community where the reference genomes are
largely known.  This data set has been used in several methods evaluations
[@awad_evaluating_2017,@ondov_mash_2019]]. (CTB add SPADes etc refs.)

![
**Letter-value plot [@hofmann_letter-value_2017] of the
differences from containment estimate to ground truth (exact).**
Each method is evaluated for $k=\{21,31,51\}$,
except for `Mash` with $k=51$, which is unsupported.
**A**: Using all 68 reference genomes found in previous articles.
**B**: Excluding low coverage genomes identified in previous articles.
](images/containment.svg "Containment estimation between smol, CMash, and mash screen"){#fig:containment}

All methods are within 1\% of the exact containment on average (Figure
@fig:containment A), with `CMash` consistently underestimating
the containment for large $k$ and overestimating for small $k$.  `Mash
Screen` with $n=10000$ has the smallest difference to ground truth for
$k=\{21, 31\}$, followed by `smol` with `scaled=1000` and `Mash
Screen` with $n=1000$.

Figure @fig:containment B shows results with low-coverage and
contaminant genomes (as described in [@awad_evaluating_2017] and
[@ondov_mash_2019]) removed from the database.  The number of outliers
is greatly reduced, with most methods within 1\% absolute difference
to the ground truth.  `CMash` still has some outliers with up to 8\%
difference to the ground truth.

CTB questions:

* should we _just_ use (B) benchmark?
* should we add sketch sizes in here more explicitly? e.g. number of hashes kept?
* compares well with others
* How much is missed figure; Poisson calculations? => appendix?

## Reference genomes can be selected for a metagenome using a simple greedy algorithm

We next ask: what is the smallest collection of genomes in a database
that should be used as a reference for k-mer based analysis of a
metagenome?  This question can be framed formally as follows: for a
given metagenome $M$ and a reference database $D$, what is the minimal
collection of genomes in $D$ which contain all of the k-mers in the
intersection of $D$ and $M$? That is, we wish to find the smallest set
$\{ G_n \}$ of genomes in $D$ such that $$k(M) \cap k(D) = \bigcup_n
\{ k(M) \cap k(G_n) \} $$

This is equivalent to the *minimal set covering* problem, for which
there is a polynomial-time approximation (cite).  (Provide algorithm here.)

For very large databases such as GenBank (which contains over 700,000
microbial genomes as of January 2021), this is computationally prohibitive
to do exactly. (Estimate total number of k-mers in genbank!) We therefore implemented the algorithm using _Scaled MinHash_
sketches to estimate containment.

The results on two metagenomes, podar (used above) and p88mo11, an IBD
data set, are shown in figure XXX. (Should we add environmental,
e.g. hu-s1?)

![
**Hash-based decomposition of a metagenome into constituent genomes**
Each figure shows a rank ordering of the genomes chosen as likely
reference genomes, based on the maximum containment approach used by
our min-set-cov implementation. The Y is labeled with the name of the
genome (per NCBI), and the red circles indicates the number of
remaining k-mers (estimated with _Scaled MinHash_) shared between each
genome and the metagenome. The green x indicate the total number of k-mers
shared between each genome and the metagenome, including those already
assigned at previous ranks. (A) Gathergram for mock metagenome from Shakya
et al. (B) Gathergram for iHMP data set p880mo11. (C) Gathergram for 
oil well data set hu-s1.
](images/gathergram-SRR606249.hashes.svg "gather hash results for podar"){#fig:gather0}

Figure @fig:gather0 shows the results of this algorithm applied to three
metagenomes - one mock, one iHMP, and one environmental. The monotonically
decreasing assignments are as expected from the algorithm, which is greedy.

Note that by using max containment, we are estimating which k-mers belong to
the genome combinatorially.  This is opposed to the way LCA approaches work.

ZZ total numbers of genomes are identified.

XX and YY percent of metagenomes are identified.

Overlapping portions of genomes are identified.

TODO:

* Provide summaries of % k-mers identified, etc.
* compare conceptually vs LCA approaches; combinatorial. do we want to
  do a benchmark of some kind wrt LCA saturation?

CTB: do we want to do this with all k-mers, not just scaled minhash?

CTB: how do we address questions of accuracy? Talking it out --

* we do this hash based decomposition
* for podar, where we know the answer, it matches previous results (sherine paper, etc?)
* we evaluate mapping next
* logically speaking, there is no way to validate this since no one else can do this scale of analysis, right?
* _maybe_ we want to switch this section more to validation on known/expected and then do mapping based validation, next section and beyond?

## K-mer foo approximates mappability

(this could be before, or after taxonomic validation?)

We evaluated `gather`s performance on the Shakya data as used above,
against GenBank, and compared the genome containment estimation with
read mapping.

K-mers have been widely used to approximate mapping. ...

We implement a mapping version of gather, in which we map all metagenome reads
to all the genomes identified by gather, and then iteratively subtract the
reads that mapped to the gather results in the order specified by gather
and remap them. This lets us compare gather results to mapping results.

Figure @fig:gather shows that mapping results do approximate gather results.
However, they do so better for synthetic communities than for real
communities, especially as gather rank increases.  This is because in
synthetic communities the reference genomes are closer to the actual
content of the metagenome, while in real metagenomes we are mapping to
imperfect references. 

In particular, both the remnant k-mer and the remnant mappings decrease
substantially with increased gather rank.
This is because at the higher ranks we
are not mapping to all elements in the genome; e.g. in figure XXX, we
see that there is a substantial difference in the total number of
bases mapped vs the leftover reads from iterative removal. Here only
reads that did not map to higher ranked genomes are mapping.

Inspection of the genome taxonomy show that in these situations, we
are mapping to subsets of genomes that are the same species or genus
as earlier ranked genomes. Figure XYZ compares the best-ranked
hash count to the aggregate hash count for the species pangenome; for
many species, the aggregate hashes identified for each
species in total far outweighs the hashes identified for any one
genome.

(belongs in discussion)
This suggests that metagenome reads are being mapped to different
genomic elements from a species pangenome. While we do not have the
resolution to determine this, the most parsimonious interpretation
is that the "true" reference genome for the species present in the
sample is not in the database, and instead is being cobbled together
from core and accessory genome elements in the database.

(Maybe this is where we use R. gnavus genomes? Yes - take JUST reads
that map to R. gnavus, do gather, show what happens x all gnavus
genomes? Could also do withholding, to show that pangenome elements will
usually map one way or another.)

(Show plots with leftover mapping vs all mapping.)

(maybe use sgc here? if so, this would be the last section!)

(CTB: revisit CMash/mash screen papers here to see how
they evaluated. Also, maybe mention sgc gbio paper and recovery of new
genome.)

![
**Hash-based decomposition of a metagenome into constituent
genomes compares well to bases covered by read mapping.** 
The reference genomes are rank ordered along the x axis based on the largest number of hashes from the metagenome specific to that genome, i.e. by order in gather output; hence the number of hashes classified for each genome (orange dots) is monotonically decreasing.
The y axis shows absolute number of estimated k-mers classified to this genome (orange) or total number of bases covered in the reference (blue); the numbers have not been rescaled.
Decreases in mapping (green peaks) occur for genomes which are not
exact matches to the genomes of the organisms used to build the mock
community (cite sherine, mash screen).
](images/gather-podar.svg "gather results for podar"){#fig:gather}

## Taxonomic profiling based on 'gather' is accurate

* CAMI results
* suggests gather/greedy decomposition is pretty good

We implement a lightweight taxonomic profiling method on top of gather
by directly transferring the taxonomies for the discovered genomes into
the profile. Lineages can then be summarized at each taxonomic rank.

To evaluate the performance of taxonomic profiling, we used the mouse
gut metagenome dataset [@meyer_tutorial_2020] from the Critical
Assessment of Metagenome Intepretation (CAMI)
[@sczyrba_critical_2017], a community-driven initiative for
reproducibly benchmarking metagenomic methods.
The simulated
mouse gut metagenome (_MGM_) was derived from 791 bacterial and
archaeal genomes,
representing 8 phyla,
18 classes,
26 orders,
50 families,
157 genera,
and 549 species.
64 samples were generated with _CAMISIM_,
with 91.8 genomes present on each sample on average.
Each sample is 5 GB in size, and both short-read (Illumina) and
long-read (PacBio) sequencing data is available.

![
Comparison per taxonomic rank of methods in terms of completeness, purity (1% filtered), and L1 norm.
](images/spider_plot_relative.svg){#fig:spider}

![
Performance per method at all major taxonomic ranks, with the shaded bands showing the standard deviation of a metric.  In **a** and **b**, completeness, purity, and L1 norm error range between 0 and 1.  The L1 norm error is normalized to this range and is also known as Bray-Curtis distance.  The higher the completeness and purity, and the lower the L1 norm, the better the profiling performance.
](images/ranks_by_tool.svg){#fig:ranks}

![
Methods rankings and scores obtained for the different metrics over all samples and taxonomic ranks.  For score calculation, all metrics were weighted equally.
](images/scores.svg){#fig:scores}


Figure @fig:spider, @fig:ranks, @fig:scores is an updated version of Figure 6 from [@meyer_tutorial_2020] including `sourmash`,
comparing 10 different methods for taxonomic profiling and their characteristics at each taxonomic rank. 
While previous methods show reduced completeness,
the ratio of taxa correctly identified in the ground truth,
below the genus level,
`sourmash` can reach 88.7\% completeness at the species level with the highest
purity (the ratio of correctly predicted taxa over all predicted taxa) across
all methods:
95.9\% when filtering predictions below 1\% abundance,
and 97\% for unfiltered results.
`sourmash` also has the lowest L1-norm error
(the sum of the absolute difference between the true and predicted abundances at
a specific taxonomic rank),
the highest number of true positives and the lowest number of false positives.

| Taxonomic binner                | Time (hh:mm) | Memory (kbytes) |
|:--------------------------------|-------------:|----------------:|
| MetaPhlAn 2.9.21                | 18:44        | 5,139,172       |
| MetaPhlAn 2.2.0                 | 12:30        | 1,741,304       |
| Bracken 2.5 (only Bracken)      | **0:01**     | **24,472**      |
| Bracken 2.5 (Kraken and Bracken)| **3:03**     | 39,439,796      |
| FOCUS 0.31                      | 13:27        | 5,236,199       |
| CAMIARKQuikr 1.0.0              | 16:19        | 27,391,555      |
| mOTUs 1.1                       | 19:50        | **1,251,296**   |
| mOTUs 2.5.1                     | 14:29        | 3,922,448       |
| MetaPalette 1.0.0               | 76:49        | 27,297,132      |
| TIPP 2.0.0                      | 151:01       | 70,789,939      |
| MetaPhyler 1.25                 | 119:30       |  2,684,720      |
| sourmash 3.4.0                  | 16:41        |  5,760,922      |

Table: Updated Supplementary Table 12 from [@meyer_tutorial_2020].
Elapsed (wall clock) time (h:mm) and maximum resident set size
(kbytes) of taxonomic profiling methods on the 64 short read samples
of the CAMI II mouse gut data set. The best results are shown in
bold. Bracken requires to run Kraken, hence the times required to run
Bracken and both tools are shown. The taxonomic profilers were run on
a computer with an Intel Xeon E5-4650 v4 CPU (virtualized to 16 CPU
cores, 1 thread per core) and 512 GB (536.870.912 kbytes) of main
memory. {#tbl:gather-cami2}

When considering resource consumption and running times, `sourmash`
used 5.62 GB of memory with an _LCA index_ built from the RefSeq
snapshot (141,677 genomes) with $scaled=10000$ and $k=51$.  Each
sample took 597 seconds to run (on average), totalling 10 hours and 37
minutes for 64 samples.  MetaPhlan 2.9.21 was also executed in the
same machine, a workstation with an AMD Ryzen 9 3900X 12-Core CPU
running at 3.80 GHz, 64 GB DDR4 2133 MHz of RAM and loading data from
an NVMe SSD, in order to compare to previously reported times in Table
@tbl:gather-cami2 [@meyer_tutorial_2020].  MetaPhlan took 11 hours and
25 minutes to run for all samples, compared to 18 hours and 44 minutes
previously reported, and correcting the `sourmash` running time by
this factor it would likely take 16 hours and 41 minutes in the
machine used in the original comparison.  After correction, `sourmash`
has similar runtime and memory consumption to the other best
performing tools (_mOTUs_ and _MetaPhlAn_), both gene marker and
alignment based tools.

Additional points are that `sourmash` is a single-threaded program, so
it didn't benefit from the 16 available CPU cores, and it is the only
tool that could use the full RefSeq snapshot, while the other tools
can only scale to a smaller fraction of it (or need custom databases).
The CAMI II RefSeq snapshot for reference genomes also doesn't include
viruses; this benefits `sourmash` because viral _Scaled MinHash_
sketches are usually not well supported for containment estimation,
since viral sequences require small scaled values to have enough
hashes to be reliable.

Notes:

* private database, private taxonomies are easily supported without reindexing.
